{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75779f7f-682a-4fc3-b78f-5206fab818b5",
   "metadata": {},
   "source": [
    "# Finetune an LLM on Amazon SageMaker\n",
    "In this notebook, we are going to focus on 3 topics:\n",
    "\n",
    "1. Process a public available dataset for LLM training/finetuning \n",
    "2. Finetune an LLM using QLoRA, an efficient finetuning technique that matches the performance of full-precision fine-tuning approaches.\n",
    "3. Deploy the finetuned LLM for inference using SageMaker.\n",
    "\n",
    "For preprocessing the dataset, we use a SageMaker Processing job to help provide the compute resources required to complete the processing steps.\n",
    "\n",
    "For model finetuning, we'll be using a SageMaker Training job to automatically spins up compute resources, execute the model training steps, and shutdown the resources automatically when the job is complete. \n",
    "\n",
    "To deploy the finetuned model, we'll be using the SageMaker Python SDK to deploy the model into SageMaker for a fully managed HTTPS endpoint in a single command.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94ee4f-d395-42d1-ae39-838e1d9854c7",
   "metadata": {},
   "source": [
    "First, we need to install the dependencies needed to run the notebook end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a433f40d-5dbb-41b6-82b4-bea95d81fe09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.13.1 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.34.153 which is incompatible.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker boto3 datasets pygments -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c8845e-6810-4872-861e-27758fc31285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::866824485776:role/service-role/AmazonSageMaker-ExecutionRole-20240725T121088\n",
      "sagemaker bucket: sagemaker-us-east-1-866824485776\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.local import LocalSession\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sagemaker.experiments.run import Run\n",
    "import uuid\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93f1dc4-2b14-44d2-ac48-526f9e9fa74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdcaa9e7-7aef-4f1f-8e8e-7cf6753a34e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff28fed-c466-4e1e-baae-788e9d59988a",
   "metadata": {},
   "source": [
    "Define the variables to be used for the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8b1461-d967-42c8-9f17-19fa5f8c951d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"base_model_pkg_group_name\" not in locals():\n",
    "    base_model_pkg_group_name = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54553e0f-ea37-42ce-9a5c-4b71bf4b944b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_s3_loc: s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/train\n",
      "validation_dataset_s3_loc: s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/eval\n",
      "model artifact S3 location: s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/model\n",
      "model evaluation output S3 location: s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/modeleval\n",
      "model_id: NousResearch/Llama-2-7b-chat-hf\n",
      "base model package group name: arn:aws:sagemaker:us-east-1:866824485776:model-package-group/NousResearch-Llama-2-7b-chat-hf\n",
      "Huggingfae dataset name: hotpot_qa\n"
     ]
    }
   ],
   "source": [
    "rand_id = uuid.uuid4().hex[:5] # this is the random-id assigned for each run. \n",
    "training_dataset_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/train\"\n",
    "validation_dataset_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/eval\"\n",
    "model_output_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/model\"\n",
    "model_eval_s3_loc = f\"s3://{sagemaker_session_bucket}/data/workshop-{rand_id}/modeleval\"\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "hf_dataset_name = \"hotpot_qa\"\n",
    "\n",
    "print(f\"training_dataset_s3_loc: {training_dataset_s3_loc}\")\n",
    "print(f\"validation_dataset_s3_loc: {validation_dataset_s3_loc}\")\n",
    "print(f\"model artifact S3 location: {model_output_s3_loc}\")\n",
    "print(f\"model evaluation output S3 location: {model_eval_s3_loc}\")\n",
    "print(f\"model_id: {model_id}\")\n",
    "print(f\"base model package group name: {base_model_pkg_group_name}\")\n",
    "print(f\"Huggingfae dataset name: {hf_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad14044-95a2-4d01-b182-1503bb449d00",
   "metadata": {},
   "source": [
    "# Proprocessing Data\n",
    "In our workshop, we'll build a generative AI chatbot application which requires the LLM the ability to understand instructions, and to provide accurate answer based on user query in natural language. \n",
    "For this reason, we choose an open source Llama2 base model [NousResearch-Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf) which has been instruction tuned. We will finetune this model using good quality Q&A dataset. \n",
    "\n",
    "For our lab, we'll use a public dataset called [hotpotQA](https://hotpotqa.github.io/) as the data source. Here's a short summary of the dataset: \n",
    "\n",
    "HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. It is collected by a team of NLP researchers at Carnegie Mellon University, Stanford University, and Université de Montréal.\n",
    "\n",
    "## SageMaker Processing\n",
    "\n",
    "To analyze data and evaluate machine learning models on Amazon SageMaker, we use a Amazon SageMaker Processing job. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. You can also use the Amazon SageMaker Processing APIs during the experimentation phase and after the code is deployed in production to evaluate performance.\n",
    "\n",
    "Here's a diagram that depicts how SageMaker Processing work:\n",
    "\n",
    "![sagemaker-processing](images/sagemaker-processing-diagram.png)\n",
    "\n",
    "In particular, we'll leverage a python script which contains the required code to handle the dataset. The script is executed in a Sagemaker processing job to automate the task end to end. The processing script can be shown in the following, and accessible in [src/preprocess/preprocess.py](src/preprocess/preprocess.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ae9c6-abe0-4fa1-bed7-a786badbf8b4",
   "metadata": {},
   "source": [
    "In the next cell, we'll process the data by running the script above as a SageMaker processing job. \n",
    "\n",
    "To launch a processing job, we use a Pytorch container by executing the `PytorchProcessor.run()` method. The `run()` method supports passing the arguments to the script.\n",
    "\n",
    "You can optionally provide input data in run() method to provide an input dataset on S3 bucket. By default, SageMaker processing job will download the data from the specified S3 location into local path inside the processing container in `/opt/ml/processing/input` directory.\n",
    "\n",
    "You could also provide an S3 location for the output data via the run() method by configuring an `ProcessingOutput` object. If not provided, SageMaker processing job defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name/`. \n",
    "\n",
    "Following code shows the python script to be used for the processing job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349acdf0-84bc-4b07-b37d-873be055c90e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mformat_hotpot\u001b[39;49;00m(sample):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that takes a single data sample derived from Huggingface datasets API: (https://huggingface.co/docs/datasets/index)\u001b[39;49;00m\n",
      "\u001b[33m    and formats it into llama2 prompt format. For more information about llama2 prompt format, \u001b[39;49;00m\n",
      "\u001b[33m    please refer to https://huggingface.co/blog/llama2#how-to-prompt-llama-2 \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    An example prompt is shown in the following:\u001b[39;49;00m\n",
      "\u001b[33m    <s>\u001b[39;49;00m\n",
      "\u001b[33m      [INST] <<SYS>>\u001b[39;49;00m\n",
      "\u001b[33m        {{system}}\u001b[39;49;00m\n",
      "\u001b[33m      <</SYS>>\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m      ### Question\u001b[39;49;00m\n",
      "\u001b[33m      {{question}}\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m      ### Context\u001b[39;49;00m\n",
      "\u001b[33m      {{context}}[/INST] {{answer}}</s>\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    @type  sample: Dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    @param sample: dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    @rtype:   string\u001b[39;49;00m\n",
      "\u001b[33m    @return:  llama2 prompt format\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    prefix = \u001b[33m\"\u001b[39;49;00m\u001b[33m<s>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    postfix = \u001b[33m\"\u001b[39;49;00m\u001b[33m</s>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    system_start_tag = \u001b[33m\"\u001b[39;49;00m\u001b[33m<<SYS>>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    system_end_tag = \u001b[33m\"\u001b[39;49;00m\u001b[33m<</SYS>>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    instruction_start_tag = \u001b[33m\"\u001b[39;49;00m\u001b[33m[INST]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    instruction_end_tag = \u001b[33m\"\u001b[39;49;00m\u001b[33m[/INST]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    context = \u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([ \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m sample[\u001b[33m'\u001b[39;49;00m\u001b[33mcontext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33msentences\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]])\u001b[37m\u001b[39;49;00m\n",
      "    system = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mGiven the following context, answer the question as accurately as possible:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    question_prompt = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m### Question\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msample[\u001b[33m'\u001b[39;49;00m\u001b[33mquestion\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    context_prompt = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m### Context\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcontext\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    prompt = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mprefix\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00minstruction_start_tag\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msystem_start_tag\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msystem\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msystem_end_tag\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mquestion_prompt\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcontext_prompt\u001b[33m}\u001b[39;49;00m\u001b[33m{\u001b[39;49;00minstruction_end_tag\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msample[\u001b[33m'\u001b[39;49;00m\u001b[33manswer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpostfix\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m prompt\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# template dataset to add prompt to each sample\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtemplate_dataset\u001b[39;49;00m(sample):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Create a field for the given sample to store formatted llama2 prompt.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    @type  sample: Dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    @param sample: Dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    @rtype:   Dataset\u001b[39;49;00m\n",
      "\u001b[33m    @return:  Dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    sample[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mformat_hotpot(sample)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m sample\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hf-dataset-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-data-split\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m:10\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval-data-split\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m:10\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReceived arguments \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args))    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    dataset = load_dataset(args.hf_dataset_name, \u001b[33m\"\u001b[39;49;00m\u001b[33mdistractor\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, split=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain[\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.train_data_split\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    new_dataset = dataset.map(template_dataset, remove_columns=\u001b[36mlist\u001b[39;49;00m(dataset.features))\u001b[37m\u001b[39;49;00m\n",
      "    training_input_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    new_dataset.save_to_disk(training_input_path)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtraining dataset uploaded to: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_input_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    eval_dataset = load_dataset(args.hf_dataset_name, \u001b[33m\"\u001b[39;49;00m\u001b[33mdistractor\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, split=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain[\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.eval_data_split\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    new_eval_dataset = dataset.map(template_dataset, remove_columns=\u001b[36mlist\u001b[39;49;00m(eval_dataset.features))\u001b[37m\u001b[39;49;00m\n",
      "    eval_input_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/eval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    new_eval_dataset.save_to_disk(eval_input_path)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33meval dataset uploaded to: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00meval_input_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/preprocess/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80da8952-740c-4d8c-baf2-cfb63f9bc5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HuggingFaceProcessor\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "\n",
    "torch_processor = PyTorchProcessor(\n",
    "    framework_version='2.0',\n",
    "    role=get_execution_role(),\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    # instance_type='local', # uncomment for local mode\n",
    "    instance_count=1,\n",
    "    base_job_name='frameworkprocessor-PT',\n",
    "    py_version=\"py310\",\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb891a82-b01f-42b2-ab11-b0c5490192b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded src/preprocess to s3://sagemaker-us-east-1-866824485776/frameworkprocessor-PT-2024-08-04-11-05-20-598/source/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-866824485776/frameworkprocessor-PT-2024-08-04-11-05-20-598/source/runproc.sh\n",
      "INFO:sagemaker:Creating processing-job with name frameworkprocessor-PT-2024-08-04-11-05-20-598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............\u001b[34mWARNING: Skipping typing as it is not installed.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 1))\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.2)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow>=15.0.0 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mCollecting requests>=2.32.2 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.66.3 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 6.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.9.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.21.2 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 51.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.5/417.5 kB 52.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.9/39.9 MB 66.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 12.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 34.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xxhash, tqdm, requests, pyarrow-hotfix, pyarrow, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.1\n",
      "    Uninstalling pyarrow-14.0.1:\n",
      "      Successfully uninstalled pyarrow-14.0.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-2.20.0 huggingface-hub-0.24.5 pyarrow-17.0.0 pyarrow-hotfix-0.6 requests-2.32.3 tqdm-4.66.5 xxhash-3.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.1 -> 24.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(hf_dataset_name='hotpot_qa', train_data_split='1:50', eval_data_split='51:100')\u001b[0m\n",
      "\u001b[34m#015Downloading builder script:   0%|          | 0.00/6.42k [00:00<?, ?B/s]#015Downloading builder script: 100%|██████████| 6.42k/6.42k [00:00<00:00, 30.4MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading readme:   0%|          | 0.00/9.19k [00:00<?, ?B/s]#015Downloading readme: 100%|██████████| 9.19k/9.19k [00:00<00:00, 38.0MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]#015Downloading data:   1%|          | 4.32M/566M [00:00<00:13, 43.2MB/s]#015Downloading data:   3%|▎         | 16.0M/566M [00:00<00:06, 86.5MB/s]#015Downloading data:   5%|▍         | 27.6M/566M [00:00<00:05, 100MB/s] #015Downloading data:   7%|▋         | 39.4M/566M [00:00<00:04, 107MB/s]#015Downloading data:   9%|▉         | 51.0M/566M [00:00<00:04, 110MB/s]#015Downloading data:  11%|█         | 62.7M/566M [00:00<00:04, 113MB/s]#015Downloading data:  13%|█▎        | 74.4M/566M [00:00<00:04, 114MB/s]#015Downloading data:  15%|█▌        | 86.1M/566M [00:00<00:04, 115MB/s]#015Downloading data:  17%|█▋        | 97.7M/566M [00:00<00:04, 115MB/s]#015Downloading data:  19%|█▉        | 109M/566M [00:01<00:03, 116MB/s] #015Downloading data:  21%|██▏       | 121M/566M [00:01<00:03, 116MB/s]#015Downloading data:  23%|██▎       | 133M/566M [00:01<00:03, 116MB/s]#015Downloading data:  26%|██▌       | 144M/566M [00:01<00:03, 116MB/s]#015Downloading data:  28%|██▊       | 156M/566M [00:01<00:03, 117MB/s]#015Downloading data:  30%|██▉       | 168M/566M [00:01<00:03, 117MB/s]#015Downloading data:  32%|███▏      | 180M/566M [00:01<00:03, 117MB/s]#015Downloading data:  34%|███▍      | 191M/566M [00:01<00:03, 117MB/s]#015Downloading data:  36%|███▌      | 203M/566M [00:01<00:03, 117MB/s]#015Downloading data:  38%|███▊      | 215M/566M [00:01<00:03, 117MB/s]#015Downloading data:  40%|███▉      | 226M/566M [00:02<00:02, 117MB/s]#015Downloading data:  42%|████▏     | 238M/566M [00:02<00:02, 117MB/s]#015Downloading data:  44%|████▍     | 250M/566M [00:02<00:02, 117MB/s]#015Downloading data:  46%|████▌     | 261M/566M [00:02<00:02, 116MB/s]#015Downloading data:  48%|████▊     | 273M/566M [00:02<00:02, 117MB/s]#015Downloading data:  50%|█████     | 285M/566M [00:02<00:02, 117MB/s]#015Downloading data:  52%|█████▏    | 297M/566M [00:02<00:02, 117MB/s]#015Downloading data:  54%|█████▍    | 308M/566M [00:02<00:02, 117MB/s]#015Downloading data:  56%|█████▋    | 320M/566M [00:02<00:02, 117MB/s]#015Downloading data:  59%|█████▊    | 332M/566M [00:02<00:02, 117MB/s]#015Downloading data:  61%|██████    | 343M/566M [00:03<00:01, 117MB/s]#015Downloading data:  63%|██████▎   | 355M/566M [00:03<00:01, 117MB/s]#015Downloading data:  65%|██████▍   | 367M/566M [00:03<00:01, 117MB/s]#015Downloading data:  67%|██████▋   | 378M/566M [00:03<00:01, 117MB/s]#015Downloading data:  69%|██████▉   | 390M/566M [00:03<00:01, 117MB/s]#015Downloading data:  71%|███████   | 402M/566M [00:03<00:01, 117MB/s]#015Downloading data:  73%|███████▎  | 413M/566M [00:03<00:01, 117MB/s]#015Downloading data:  75%|███████▌  | 425M/566M [00:03<00:01, 117MB/s]#015Downloading data:  77%|███████▋  | 437M/566M [00:03<00:01, 117MB/s]#015Downloading data:  79%|███████▉  | 449M/566M [00:03<00:01, 117MB/s]#015Downloading data:  81%|████████  | 460M/566M [00:04<00:00, 113MB/s]#015Downloading data:  83%|████████▎ | 472M/566M [00:04<00:00, 115MB/s]#015Downloading data:  86%|████████▌ | 484M/566M [00:04<00:00, 117MB/s]#015Downloading data:  88%|████████▊ | 497M/566M [00:04<00:00, 118MB/s]#015Downloading data:  90%|████████▉ | 508M/566M [00:04<00:00, 118MB/s]#015Downloading data:  92%|█████████▏| 520M/566M [00:04<00:00, 118MB/s]#015Downloading data:  94%|█████████▍| 532M/566M [00:04<00:00, 117MB/s]#015Downloading data:  96%|█████████▌| 544M/566M [00:04<00:00, 117MB/s]#015Downloading data:  98%|█████████▊| 555M/566M [00:04<00:00, 117MB/s]#015Downloading data: 100%|██████████| 566M/566M [00:04<00:00, 115MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data:   0%|          | 0.00/46.3M [00:00<?, ?B/s]#015Downloading data:   9%|▉         | 4.23M/46.3M [00:00<00:00, 42.3MB/s]#015Downloading data:  34%|███▍      | 15.9M/46.3M [00:00<00:00, 86.2MB/s]#015Downloading data:  60%|█████▉    | 27.6M/46.3M [00:00<00:00, 100MB/s] #015Downloading data:  85%|████████▍ | 39.3M/46.3M [00:00<00:00, 107MB/s]#015Downloading data: 100%|██████████| 46.3M/46.3M [00:00<00:00, 101MB/s]\u001b[0m\n",
      "\u001b[34m#015Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]#015Generating train split:   0%|          | 1/90447 [00:05<142:00:06,  5.65s/ examples]#015Generating train split:   1%|          | 579/90447 [00:05<10:29, 142.66 examples/s] #015Generating train split:   1%|          | 1000/90447 [00:05<05:21, 278.45 examples/s]#015Generating train split:   2%|▏         | 1593/90447 [00:05<02:45, 538.03 examples/s]#015Generating train split:   3%|▎         | 2297/90447 [00:06<01:37, 900.95 examples/s]#015Generating train split:   3%|▎         | 2880/90447 [00:06<01:08, 1284.75 examples/s]#015Generating train split:   4%|▍         | 3587/90447 [00:06<00:49, 1745.25 examples/s]#015Generating train split:   5%|▍         | 4301/90447 [00:06<00:39, 2205.76 examples/s]#015Generating train split:   5%|▌         | 4894/90447 [00:06<00:31, 2708.27 examples/s]#015Generating train split:   6%|▌         | 5400/90447 [00:06<00:30, 2815.53 examples/s]#015Generating train split:   7%|▋         | 5953/90447 [00:06<00:25, 3286.00 examples/s]#015Generating train split:   7%|▋         | 6578/90447 [00:07<00:24, 3487.40 examples/s]#015Generating train split:   8%|▊         | 7292/90447 [00:07<00:22, 3709.69 examples/s]#015Generating train split:   9%|▊         | 7879/90447 [00:07<00:19, 4147.98 examples/s]#015Generating train split:   9%|▉         | 8575/90447 [00:07<00:19, 4132.50 examples/s]#015Generating train split:  10%|█         | 9241/90447 [00:07<00:20, 3963.30 examples/s]#015Generating train split:  11%|█         | 9719/90447 [00:07<00:19, 4131.54 examples/s]#015Generating train split:  11%|█▏        | 10278/90447 [00:07<00:20, 3924.98 examples/s]#015Generating train split:  12%|█▏        | 10897/90447 [00:08<00:19, 3982.82 examples/s]#015Generating train split:  13%|█▎        | 11434/90447 [00:08<00:21, 3602.04 examples/s]#015Generating train split:  13%|█▎        | 12000/90447 [00:08<00:21, 3696.15 examples/s]#015Generating train split:  14%|█▍        | 12588/90447 [00:08<00:18, 4165.15 examples/s]#015Generating train split:  15%|█▍        | 13295/90447 [00:08<00:18, 4178.96 examples/s]#015Generating train split:  15%|█▌        | 13882/90447 [00:08<00:16, 4558.47 examples/s]#015Generating train split:  16%|█▌        | 14593/90447 [00:08<00:17, 4457.12 examples/s]#015Generating train split:  17%|█▋        | 15295/90447 [00:09<00:17, 4386.26 examples/s]#015Generating train split:  18%|█▊        | 15888/90447 [00:09<00:15, 4726.85 examples/s]#015Generating train split:  18%|█▊        | 16584/90447 [00:09<00:16, 4533.31 examples/s]#015Generating train split:  19%|█▉        | 17301/90447 [00:09<00:16, 4438.27 examples/s]#015Generating train split:  20%|█▉        | 17883/90447 [00:09<00:15, 4741.78 examples/s]#015Generating train split:  21%|██        | 18577/90447 [00:09<00:15, 4550.50 examples/s]#015Generating train split:  21%|██▏       | 19300/90447 [00:10<00:15, 4448.47 examples/s]#015Generating train split:  22%|██▏       | 19887/90447 [00:10<00:14, 4757.50 examples/s]#015Generating train split:  23%|██▎       | 20582/90447 [00:10<00:15, 4572.30 examples/s]#015Generating train split:  24%|██▎       | 21297/90447 [00:10<00:15, 4496.00 examples/s]#015Generating train split:  24%|██▍       | 21883/90447 [00:10<00:14, 4796.37 examples/s]#015Generating train split:  25%|██▍       | 22591/90447 [00:11<00:27, 2449.39 examples/s]#015Generating train split:  25%|██▌       | 23000/90447 [00:11<00:25, 2601.53 examples/s]#015Generating train split:  26%|██▌       | 23587/90447 [00:11<00:21, 3119.76 examples/s]#015Generating train split:  27%|██▋       | 24296/90447 [00:11<00:19, 3405.85 examples/s]#015Generating train split:  28%|██▊       | 24881/90447 [00:11<00:16, 3866.30 examples/s]#015Generating train split:  28%|██▊       | 25583/90447 [00:11<00:16, 3987.80 examples/s]#015Generating train split:  29%|██▉       | 26296/90447 [00:11<00:15, 4088.24 examples/s]#015Generating train split:  30%|██▉       | 26884/90447 [00:12<00:14, 4461.64 examples/s]#015Generating train split:  31%|███       | 27590/90447 [00:12<00:14, 4349.35 examples/s]#015Generating train split:  31%|███▏      | 28293/90447 [00:12<00:14, 4270.00 examples/s]#015Generating train split:  32%|███▏      | 28864/90447 [00:12<00:13, 4574.28 examples/s]#015Generating train split:  33%|███▎      | 29586/90447 [00:12<00:13, 4462.26 examples/s]#015Generating train split:  33%|███▎      | 30294/90447 [00:12<00:13, 4375.48 examples/s]#015Generating train split:  34%|███▍      | 30875/90447 [00:12<00:12, 4685.06 examples/s]#015Generating train split:  35%|███▍      | 31564/90447 [00:13<00:13, 4451.63 examples/s]#015Generating train split:  36%|███▌      | 32293/90447 [00:13<00:13, 4423.43 examples/s]#015Generating train split:  36%|███▋      | 32876/90447 [00:13<00:12, 4728.53 examples/s]#015Generating train split:  37%|███▋      | 33586/90447 [00:13<00:12, 4525.70 examples/s]#015Generating train split:  38%|███▊      | 34296/90447 [00:13<00:12, 4418.38 examples/s]#015Generating train split:  39%|███▊      | 34875/90447 [00:13<00:11, 4714.91 examples/s]#015Generating train split:  39%|███▉      | 35585/90447 [00:14<00:12, 4519.85 examples/s]#015Generating train split:  40%|████      | 36301/90447 [00:14<00:12, 4509.41 examples/s]#015Generating train split:  41%|████      | 36895/90447 [00:14<00:11, 4821.93 examples/s]#015Generating train split:  42%|████▏     | 37591/90447 [00:14<00:11, 4661.71 examples/s]#015Generating train split:  42%|████▏     | 38300/90447 [00:14<00:11, 4547.95 examples/s]#015Generating train split:  43%|████▎     | 38889/90447 [00:14<00:10, 4844.72 examples/s]#015Generating train split:  44%|████▍     | 39582/90447 [00:14<00:11, 4607.92 examples/s]#015Generating train split:  45%|████▍     | 40299/90447 [00:15<00:11, 4480.47 examples/s]#015Generating train split:  45%|████▌     | 40884/90447 [00:15<00:10, 4781.01 examples/s]#015Generating train split:  46%|████▌     | 41590/90447 [00:15<00:10, 4572.30 examples/s]#015Generating train split:  47%|████▋     | 42297/90447 [00:15<00:10, 4432.28 examples/s]#015Generating train split:  47%|████▋     | 42883/90447 [00:15<00:10, 4741.62 examples/s]#015Generating train split:  48%|████▊     | 43580/90447 [00:15<00:10, 4514.50 examples/s]#015Generating train split:  49%|████▉     | 44294/90447 [00:15<00:10, 4440.87 examples/s]#015Generating train split:  50%|████▉     | 44856/90447 [00:16<00:09, 4695.97 examples/s]#015Generating train split:  50%|█████     | 45589/90447 [00:16<00:09, 4503.63 examples/s]#015Generating train split:  51%|█████     | 46295/90447 [00:16<00:10, 4385.22 examples/s]#015Generating train split:  52%|█████▏    | 46879/90447 [00:16<00:09, 4696.88 examples/s]#015Generating train split:  53%|█████▎    | 47586/90447 [00:16<00:09, 4496.60 examples/s]#015Generating train split:  53%|█████▎    | 48296/90447 [00:16<00:09, 4378.66 examples/s]#015Generating train split:  54%|█████▍    | 48881/90447 [00:16<00:08, 4694.58 examples/s]#015Generating train split:  55%|█████▍    | 49584/90447 [00:17<00:09, 4493.05 examples/s]#015Generating train split:  56%|█████▌    | 50295/90447 [00:17<00:09, 4386.17 examples/s]#015Generating train split:  56%|█████▋    | 50883/90447 [00:17<00:08, 4707.84 examples/s]#015Generating train split:  57%|█████▋    | 51582/90447 [00:17<00:08, 4567.93 examples/s]#015Generating train split:  58%|█████▊    | 52296/90447 [00:17<00:08, 4514.52 examples/s]#015Generating train split:  58%|█████▊    | 52879/90447 [00:17<00:07, 4802.99 examples/s]#015Generating train split:  59%|█████▉    | 53585/90447 [00:17<00:08, 4563.15 examples/s]#015Generating train split:  60%|██████    | 54295/90447 [00:18<00:08, 4435.21 examples/s]#015Generating train split:  61%|██████    | 54883/90447 [00:18<00:07, 4749.95 examples/s]#015Generating train split:  61%|██████▏   | 55587/90447 [00:18<00:07, 4512.45 examples/s]#015Generating train split:  62%|██████▏   | 56299/90447 [00:18<00:07, 4418.30 examples/s]#015Generating train split:  63%|██████▎   | 56889/90447 [00:18<00:07, 4737.28 examples/s]#015Generating train split:  64%|██████▎   | 57585/90447 [00:18<00:07, 4529.51 examples/s]#015Generating train split:  64%|██████▍   | 58292/90447 [00:18<00:07, 4409.59 examples/s]#015Generating train split:  65%|██████▌   | 58886/90447 [00:19<00:06, 4741.67 examples/s]#015Generating train split:  66%|██████▌   | 59588/90447 [00:19<00:06, 4536.89 examples/s]#015Generating train split:  67%|██████▋   | 60295/90447 [00:19<00:12, 2413.74 examples/s]#015Generating train split:  67%|██████▋   | 60878/90447 [00:19<00:10, 2864.44 examples/s]#015Generating train split:  68%|██████▊   | 61581/90447 [00:20<00:09, 3168.75 examples/s]#015Generating train split:  69%|██████▉   | 62302/90447 [00:20<00:08, 3448.77 examples/s]#015Generating train split:  70%|██████▉   | 62889/90447 [00:20<00:07, 3878.98 examples/s]#015Generating train split:  70%|███████   | 63580/90447 [00:20<00:06, 3942.92 examples/s]#015Generating train split:  71%|███████   | 64291/90447 [00:20<00:06, 4016.87 examples/s]#015Generating train split:  72%|███████▏  | 64871/90447 [00:20<00:05, 4375.98 examples/s]#015Generating train split:  73%|███████▎  | 65584/90447 [00:20<00:05, 4364.83 examples/s]#015Generating train split:  73%|███████▎  | 66292/90447 [00:21<00:05, 4260.64 examples/s]#015Generating train split:  74%|███████▍  | 66871/90447 [00:21<00:05, 4581.97 examples/s]#015Generating train split:  75%|███████▍  | 67578/90447 [00:21<00:05, 4362.08 examples/s]#015Generating train split:  76%|███████▌  | 68295/90447 [00:21<00:05, 4296.90 examples/s]#015Generating train split:  76%|███████▌  | 68873/90447 [00:21<00:04, 4608.93 examples/s]#015Generating train split:  77%|███████▋  | 69565/90447 [00:21<00:04, 4397.47 examples/s]#015Generating train split:  78%|███████▊  | 70292/90447 [00:22<00:04, 4303.47 examples/s]#015Generating train split:  78%|███████▊  | 70872/90447 [00:22<00:04, 4621.06 examples/s]#015Generating train split:  79%|███████▉  | 71584/90447 [00:22<00:04, 4474.94 examples/s]#015Generating train split:  80%|███████▉  | 72289/90447 [00:22<00:04, 4331.86 examples/s]#015Generating train split:  81%|████████  | 72867/90447 [00:22<00:03, 4640.20 examples/s]#015Generating train split:  81%|████████▏ | 73576/90447 [00:22<00:03, 4466.83 examples/s]#015Generating train split:  82%|████████▏ | 74293/90447 [00:22<00:03, 4420.37 examples/s]#015Generating train split:  83%|████████▎ | 74869/90447 [00:23<00:03, 4709.17 examples/s]#015Generating train split:  84%|████████▎ | 75588/90447 [00:23<00:03, 4521.77 examples/s]#015Generating train split:  84%|████████▍ | 76293/90447 [00:23<00:03, 4403.98 examples/s]#015Generating train split:  85%|████████▍ | 76871/90447 [00:23<00:02, 4701.03 examples/s]#015Generating train split:  86%|████████▌ | 77590/90447 [00:23<00:02, 4614.75 examples/s]#015Generating train split:  87%|████████▋ | 78293/90447 [00:23<00:02, 4466.31 examples/s]#015Generating train split:  87%|████████▋ | 78875/90447 [00:23<00:02, 4760.78 examples/s]#015Generating train split:  88%|████████▊ | 79588/90447 [00:24<00:02, 4554.49 examples/s]#015Generating train split:  89%|████████▉ | 80292/90447 [00:24<00:02, 4411.08 examples/s]#015Generating train split:  89%|████████▉ | 80878/90447 [00:24<00:02, 4724.98 examples/s]#015Generating train split:  90%|█████████ | 81590/90447 [00:24<00:01, 4524.44 examples/s]#015Generating train split:  91%|█████████ | 82291/90447 [00:24<00:01, 4371.43 examples/s]#015Generating train split:  92%|█████████▏| 82874/90447 [00:24<00:01, 4685.03 examples/s]#015Generating train split:  92%|█████████▏| 83581/90447 [00:24<00:01, 4534.69 examples/s]#015Generating train split:  93%|█████████▎| 84294/90447 [00:25<00:01, 4441.46 examples/s]#015Generating train split:  94%|█████████▍| 84878/90447 [00:25<00:01, 4745.23 examples/s]#015Generating train split:  95%|█████████▍| 85587/90447 [00:25<00:01, 4516.10 examples/s]#015Generating train split:  95%|█████████▌| 86294/90447 [00:25<00:00, 4436.06 examples/s]#015Generating train split:  96%|█████████▌| 86877/90447 [00:25<00:00, 4739.03 examples/s]#015Generating train split:  97%|█████████▋| 87574/90447 [00:25<00:00, 4504.13 examples/s]#015Generating train split:  98%|█████████▊| 88284/90447 [00:26<00:00, 4377.99 examples/s]#015Generating train split:  98%|█████████▊| 88860/90447 [00:26<00:00, 4675.67 examples/s]#015Generating train split:  99%|█████████▉| 89585/90447 [00:26<00:00, 4489.81 examples/s]#015Generating train split: 100%|█████████▉| 90297/90447 [00:26<00:00, 4341.82 examples/s]#015Generating train split: 100%|██████████| 90447/90447 [00:26<00:00, 3354.07 examples/s]\u001b[0m\n",
      "\u001b[34m#015Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]#015Generating validation split:   0%|          | 1/7405 [00:00<42:52,  2.88 examples/s]#015Generating validation split:   8%|▊         | 581/7405 [00:00<00:04, 1691.35 examples/s]#015Generating validation split:  14%|█▎        | 1000/7405 [00:00<00:02, 2329.89 examples/s]#015Generating validation split:  21%|██▏       | 1586/7405 [00:00<00:02, 2703.27 examples/s]#015Generating validation split:  27%|██▋       | 2000/7405 [00:00<00:01, 2963.23 examples/s]#015Generating validation split:  35%|███▍      | 2568/7405 [00:00<00:01, 3658.26 examples/s]#015Generating validation split:  41%|████      | 3000/7405 [00:01<00:01, 3620.58 examples/s]#015Generating validation split:  49%|████▊     | 3596/7405 [00:01<00:00, 4240.68 examples/s]#015Generating validation split:  58%|█████▊    | 4290/7405 [00:01<00:00, 4227.44 examples/s]#015Generating validation split:  66%|██████▌   | 4871/7405 [00:01<00:00, 4620.36 examples/s]#015Generating validation split:  76%|███████▌  | 5591/7405 [00:01<00:00, 4477.36 examples/s]#015Generating validation split:  85%|████████▍ | 6289/7405 [00:01<00:00, 4374.50 examples/s]#015Generating validation split:  92%|█████████▏| 6839/7405 [00:01<00:00, 4627.90 examples/s]#015Generating validation split: 100%|██████████| 7405/7405 [00:02<00:00, 4130.22 examples/s]#015Generating validation split: 100%|██████████| 7405/7405 [00:02<00:00, 3564.92 examples/s]\u001b[0m\n",
      "\u001b[34m#015Map:   0%|          | 0/49 [00:00<?, ? examples/s]#015Map: 100%|██████████| 49/49 [00:00<00:00, 3365.33 examples/s]\u001b[0m\n",
      "\u001b[34m#015Saving the dataset (0/1 shards):   0%|          | 0/49 [00:00<?, ? examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 17081.19 examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 16104.13 examples/s]\u001b[0m\n",
      "\u001b[34mtraining dataset uploaded to: /opt/ml/processing/train\u001b[0m\n",
      "\u001b[34m#015Map:   0%|          | 0/49 [00:00<?, ? examples/s]#015Map: 100%|██████████| 49/49 [00:00<00:00, 3491.81 examples/s]\u001b[0m\n",
      "\u001b[34m#015Saving the dataset (0/1 shards):   0%|          | 0/49 [00:00<?, ? examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 17865.17 examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 16730.78 examples/s]\u001b[0m\n",
      "\u001b[34meval dataset uploaded to: /opt/ml/processing/eval\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    source_dir=\"src/preprocess\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\",\n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=training_dataset_s3_loc),\n",
    "        ProcessingOutput(output_name=\"eval_data\",\n",
    "                         source=\"/opt/ml/processing/eval\",\n",
    "                         destination=validation_dataset_s3_loc),\n",
    "\n",
    "    ],\n",
    "    arguments=[\"--train-data-split\", \"1:50\",\n",
    "               \"--eval-data-split\", \"51:100\",\n",
    "               \"--hf-dataset-name\", hf_dataset_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f0c16-0580-4b54-b918-90ec46326a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-Tune Llama2-7b model on Amazon SageMaker\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. \n",
    "QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. \n",
    "The TL;DR; of how QLoRA works is:\n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a train.py, which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code.\n",
    "\n",
    "Here's an animation that shows how how QLoRA works in general.\n",
    "\n",
    "![lora-animated](images/lora-animated.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed79eaf-b53d-4c78-b3a4-4fe04acf02e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    AutoModelForCausalLM,\u001b[37m\u001b[39;49;00m\n",
      "    AutoTokenizer,\u001b[37m\u001b[39;49;00m\n",
      "    BitsAndBytesConfig,\u001b[37m\u001b[39;49;00m\n",
      "    HfArgumentParser,\u001b[37m\u001b[39;49;00m\n",
      "    TrainingArguments,\u001b[37m\u001b[39;49;00m\n",
      "    pipeline,\u001b[37m\u001b[39;49;00m\n",
      "    logging,\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpeft\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LoraConfig, PeftModel\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtrl\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SFTTrainer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mhuggingface\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m HuggingFaceModel\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcollection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Collection\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtqdm\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m tqdm\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msmexperiments_callback\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerExperimentsCallback\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mbotocore\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexceptions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClientError\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36murllib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m urlparse\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mbitsandbytes\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mbnb\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Output directory where the model predictions and checkpoints will be stored\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "output_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "base_model_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/basemodel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "model_eval_save_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/eval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Load the entire model on the GPU 0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "device_map = \u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_arge\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Parse the arguments.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mgoogle/flan-t5-xl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mModel id to use for training.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of epochs to train for.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,  help=\u001b[33m\"\u001b[39;49;00m\u001b[33mUse fp16 for training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mper device training batch size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mper device evaluation batch size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--gradient_accumulation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of batches to accumulate for gradients before optimization step\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--gradient_checkpointing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mapply gradient checkpointing for moemory optimization at training time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_grad_norm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.3\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgradient norm clipping value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m2e-4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--weight_decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mweight decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mpaged_adamw_32bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moptimizer to use\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr_scheduler_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mconstant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate scheduler type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=-\u001b[34m1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmaximum steps to train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.03\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate warm up ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--group_by_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgroupping datase by length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m25\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of training steps before saving a checkpoint\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--logging_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m25\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of steps before logging the training metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmaximum sequence length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--packing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mpack multiple examples into input sequence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_r\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mLoRA attention dimension\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_alpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mAlpha parameter for LoRA scaling\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_dropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mDropout probability for LoRA layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_4bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mWhether to use 4bit quantization\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--bnb_4bit_compute_dtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mcompute dtype for bnb_4bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--bnb_4bit_quant_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mnf4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mQuantization type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_nested_quant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mactivate nested quantization for 4bit base models (double quantization\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_bias\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to use bias in the lora adapter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--merge_weights\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mWhether to merge LoRA weights with base model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--base_model_group_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mOptional base model group name.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--region\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mus-east-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mthe region where the training job is run.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm_train_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm_validation_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_eval_s3_loc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_experiment\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    args = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m args\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32ms3_download\u001b[39;49;00m(s3_bucket, s3_object_key, local_file_name, s3_client=boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that downloads an object from S3 into local filesystem using boto3 library.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    meta_data = s3_client.head_object(Bucket=s3_bucket, Key=s3_object_key)\u001b[37m\u001b[39;49;00m\n",
      "    total_length = \u001b[36mint\u001b[39;49;00m(meta_data.get(\u001b[33m'\u001b[39;49;00m\u001b[33mContentLength\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m tqdm(total=total_length,  \u001b[37m\u001b[39;49;00m\n",
      "              desc=\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33msource: s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00ms3_bucket\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00ms3_object_key\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              bar_format=\u001b[33m\"\u001b[39;49;00m\u001b[33m{percentage:.1f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m{bar:25}\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{rate_fmt}\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,  \u001b[37m\u001b[39;49;00m\n",
      "              unit=\u001b[33m'\u001b[39;49;00m\u001b[33mB\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              unit_scale=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              unit_divisor=\u001b[34m1024\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "             ) \u001b[34mas\u001b[39;49;00m pbar:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(local_file_name, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "            s3_client.download_fileobj(s3_bucket, s3_object_key, f, Callback=pbar.update)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdownload_and_untar_s3_tar\u001b[39;49;00m(destination_path, source_s3_path):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that downloads a file on S3, then untar the file in local filesystem.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    src_s3_bucket = source_s3_path.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m2\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    src_s3_prefix = \u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(source_s3_path.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m3\u001b[39;49;00m:])\u001b[37m\u001b[39;49;00m\n",
      "    destination_file_path = os.path.join(destination_path, os.path.basename(source_s3_path))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mDownloading file from \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msrc_s3_bucket\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msrc_s3_prefix\u001b[33m}\u001b[39;49;00m\u001b[33m to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdestination_file_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    s3_download(\u001b[37m\u001b[39;49;00m\n",
      "        s3_bucket=src_s3_bucket,\u001b[37m\u001b[39;49;00m\n",
      "        s3_object_key=src_s3_prefix,\u001b[37m\u001b[39;49;00m\n",
      "        local_file_name=destination_file_path\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Create a tarfile object and extract the contents to the local disk\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tar = tarfile.open(destination_file_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    tar.extractall(path=destination_path)\u001b[37m\u001b[39;49;00m\n",
      "    tar.close()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_data_uri_from_model_package\u001b[39;49;00m(model_group_name, region=\u001b[33m\"\u001b[39;49;00m\u001b[33mus-east-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that retrieves the model artifact for the given model group name.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    sagemaker_session = sagemaker.session.Session(boto3.session.Session(region_name=region))\u001b[37m\u001b[39;49;00m\n",
      "    region = sagemaker_session.boto_region_name\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    sm_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_packages = sm_client.list_model_packages(\u001b[37m\u001b[39;49;00m\n",
      "        ModelPackageGroupName=model_group_name\u001b[37m\u001b[39;49;00m\n",
      "    )[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageSummaryList\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_package_name = \u001b[36msorted\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        [\u001b[37m\u001b[39;49;00m\n",
      "            (package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageVersion\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageArn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]) \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m package \u001b[35min\u001b[39;49;00m model_packages \u001b[34mif\u001b[39;49;00m package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelApprovalStatus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33mApproved\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "        reverse=\u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )[-\u001b[34m1\u001b[39;49;00m][-\u001b[34m1\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mfound model package: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_package_name\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m sm_client.describe_model_package(\u001b[37m\u001b[39;49;00m\n",
      "        ModelPackageName=model_package_name\u001b[37m\u001b[39;49;00m\n",
      "    )[\u001b[33m'\u001b[39;49;00m\u001b[33mInferenceSpecification\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mContainers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mModelDataUrl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mquantization_config\u001b[39;49;00m(args, compute_dtype):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    At a high level, QLoRA uses 4-bit quantization to compress a pretrained language model.\u001b[39;49;00m\n",
      "\u001b[33m    This function sets up the quantization configuration for model training with QLoRA. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    The LoRA layers are the only parameters being updated during training. \u001b[39;49;00m\n",
      "\u001b[33m    Read more about LoRA in the original LoRA paper (https://arxiv.org/abs/2106.09685). \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# 4 bit configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    bnb_config = BitsAndBytesConfig(\u001b[37m\u001b[39;49;00m\n",
      "        load_in_4bit=args.use_4bit,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_compute_dtype=compute_dtype,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_use_double_quant=args.use_nested_quant,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m bnb_config\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_pretrained_model\u001b[39;49;00m(args, model_name, compute_dtype):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads the pretrained model into GPU memory for finetuning. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    There are 2 modes supported: \u001b[39;49;00m\n",
      "\u001b[33m    1. Directly download a pretrained model weights\u001b[39;49;00m\n",
      "\u001b[33m    from Huggingface Hub over the public internet.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    2. Download the pretrained model weight from a base model package\u001b[39;49;00m\n",
      "\u001b[33m    registered in SageMaker Model Registry. Downloading weights from S3 \u001b[39;49;00m\n",
      "\u001b[33m    could improve the download speed significantly. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.base_model_group_name != \u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        os.makedirs(base_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        model_data_uri = model_data_uri_from_model_package(\u001b[37m\u001b[39;49;00m\n",
      "            model_group_name=args.base_model_group_name,\u001b[37m\u001b[39;49;00m\n",
      "            region=args.region\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        download_and_untar_s3_tar(\u001b[37m\u001b[39;49;00m\n",
      "        destination_path=base_model_path, \u001b[37m\u001b[39;49;00m\n",
      "        source_s3_path=model_data_uri\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    bnb_config = quantization_config(args, compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load base model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForCausalLM.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_name,\u001b[37m\u001b[39;49;00m\n",
      "        quantization_config=bnb_config,\u001b[37m\u001b[39;49;00m\n",
      "        device_map=device_map\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model.config.use_cache = \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model.config.pretraining_tp = \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_tokenizer\u001b[39;49;00m(args, model_name):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads the tokenizer for llama2 model. Tokenizer is used in the training process to \u001b[39;49;00m\n",
      "\u001b[33m    convert the input texts into tokens. \u001b[39;49;00m\n",
      "\u001b[33m    Please refer to: https://huggingface.co/docs/transformers/v4.31.0/model_doc/llama2#transformers.LlamaTokenizer \u001b[39;49;00m\n",
      "\u001b[33m    to learn more about this specific tokenizer.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.pad_token = tokenizer.eos_token\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.padding_side = \u001b[33m\"\u001b[39;49;00m\u001b[33mright\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m# Fix weird overflow issue with fp16 training\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m tokenizer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_lora_config\u001b[39;49;00m(args, modules):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads QLoRA configuration for the training job. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load LoRA configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    peft_config = LoraConfig(\u001b[37m\u001b[39;49;00m\n",
      "        lora_alpha=args.lora_alpha,\u001b[37m\u001b[39;49;00m\n",
      "        lora_dropout=args.lora_dropout,\u001b[37m\u001b[39;49;00m\n",
      "        r=args.lora_r,\u001b[37m\u001b[39;49;00m\n",
      "        bias=args.lora_bias,\u001b[37m\u001b[39;49;00m\n",
      "        task_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mCAUSAL_LM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        target_modules = modules\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m peft_config\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32ms3_upload\u001b[39;49;00m(model_evaluation_s3_path, local_file_name, s3_client=boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Uploads the given file in local file system to a specified S3 location.\u001b[39;49;00m\n",
      "\u001b[33m    This function is used for uploading the model evaluation metrics to S3.\u001b[39;49;00m\n",
      "\u001b[33m    The metrics will be used when registering the trained model with SageMaker Model Registry.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    o = urlparse(model_evaluation_s3_path)\u001b[37m\u001b[39;49;00m\n",
      "    s3_bucket = o.netloc\u001b[37m\u001b[39;49;00m\n",
      "    s3_object_key = o.path\u001b[37m\u001b[39;49;00m\n",
      "    local_base_file_name = os.path.basename(local_file_name)\u001b[37m\u001b[39;49;00m\n",
      "    s3_object_key = os.path.join(s3_object_key, local_base_file_name)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        response = s3_client.upload_file(local_file_name, s3_bucket, s3_object_key.lstrip(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m ClientError \u001b[34mas\u001b[39;49;00m e:\u001b[37m\u001b[39;49;00m\n",
      "        logging.error(e)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_evaluation\u001b[39;49;00m(args, metrics):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Captures the training and evaluation metrics from the model training exercise. \u001b[39;49;00m\n",
      "\u001b[33m    The metrics will be written to file, and copied to the specifiued S3 locaiton.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train_loss = \u001b[36mfloat\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    eval_loss = \u001b[36mfloat\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m metric \u001b[35min\u001b[39;49;00m metrics:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m metric:\u001b[37m\u001b[39;49;00m\n",
      "            train_loss = metric[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m metric:\u001b[37m\u001b[39;49;00m\n",
      "            eval_loss = metric[\u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_metrics =  {\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mregression_metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : {\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_loss\u001b[37m\u001b[39;49;00m\n",
      "            },\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : {\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : eval_loss\u001b[37m\u001b[39;49;00m\n",
      "            }\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mevaluation_metrics\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Save Evaluation Report\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    pathlib.Path(model_eval_save_dir).mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_path = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_eval_save_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "        f.write(json.dumps(evaluation_metrics))\u001b[37m\u001b[39;49;00m\n",
      "    s3_upload(args.model_eval_s3_loc, evaluation_path)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# COPIED FROM https://github.com/artidoro/qlora/blob/main/qlora.py\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfind_all_linear_names\u001b[39;49;00m(model):\u001b[37m\u001b[39;49;00m\n",
      "    lora_module_names = \u001b[36mset\u001b[39;49;00m()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m name, module \u001b[35min\u001b[39;49;00m model.named_modules():\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(module, bnb.nn.Linear4bit):\u001b[37m\u001b[39;49;00m\n",
      "            names = name.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            lora_module_names.add(names[\u001b[34m0\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(names) == \u001b[34m1\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m names[-\u001b[34m1\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mlm_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m lora_module_names:  \u001b[37m# needed for 16-bit\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        lora_module_names.remove(\u001b[33m\"\u001b[39;49;00m\u001b[33mlm_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mlist\u001b[39;49;00m(lora_module_names)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtraining_function\u001b[39;49;00m(args):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mmerging weights: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.merge_weights\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.sm_train_dir)\u001b[37m\u001b[39;49;00m\n",
      "    eval_dataset = load_from_disk(args.sm_validation_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load tokenizer and model with QLoRA configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    compute_dtype = \u001b[36mgetattr\u001b[39;49;00m(torch, args.bnb_4bit_compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "    model_name = args.model_id\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.base_model_group_name != \u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        model_name = base_model_path\u001b[37m\u001b[39;49;00m\n",
      "    model = load_pretrained_model(args, model_name, compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = load_tokenizer(args, model_name)\u001b[37m\u001b[39;49;00m\n",
      "    lora_modules = find_all_linear_names(model)\u001b[37m\u001b[39;49;00m\n",
      "    lora_config = load_lora_config(args, lora_modules)\u001b[37m\u001b[39;49;00m\n",
      "    packing = \u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.packing \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    fp16 = \u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.fp16 \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    bf16 = \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m compute_dtype == torch.float16 \u001b[35mand\u001b[39;49;00m args.use_4bit:\u001b[37m\u001b[39;49;00m\n",
      "        major, _ = torch.cuda.get_device_capability()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m major >= \u001b[34m8\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * \u001b[34m80\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mYour GPU supports bfloat16: accelerate training with bf16=True\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * \u001b[34m80\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            bf16=\u001b[34mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set training parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    training_arguments = TrainingArguments(\u001b[37m\u001b[39;49;00m\n",
      "        output_dir=output_dir,\u001b[37m\u001b[39;49;00m\n",
      "        num_train_epochs=args.epochs,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_train_batch_size=args.per_device_train_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        gradient_accumulation_steps=args.gradient_accumulation_steps,\u001b[37m\u001b[39;49;00m\n",
      "        optim=args.optimizer,\u001b[37m\u001b[39;49;00m\n",
      "        save_steps=args.save_steps,\u001b[37m\u001b[39;49;00m\n",
      "        logging_steps=args.logging_steps,\u001b[37m\u001b[39;49;00m\n",
      "        learning_rate=args.learning_rate,\u001b[37m\u001b[39;49;00m\n",
      "        weight_decay=args.weight_decay,\u001b[37m\u001b[39;49;00m\n",
      "        fp16=fp16,\u001b[37m\u001b[39;49;00m\n",
      "        bf16=bf16,\u001b[37m\u001b[39;49;00m\n",
      "        max_grad_norm=args.max_grad_norm,\u001b[37m\u001b[39;49;00m\n",
      "        max_steps=args.max_steps,\u001b[37m\u001b[39;49;00m\n",
      "        warmup_ratio=args.warmup_ratio,\u001b[37m\u001b[39;49;00m\n",
      "        group_by_length=args.group_by_length,\u001b[37m\u001b[39;49;00m\n",
      "        lr_scheduler_type=args.lr_scheduler_type,\u001b[37m\u001b[39;49;00m\n",
      "        logging_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        report_to=\u001b[33m\"\u001b[39;49;00m\u001b[33mtensorboard\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set supervised fine-tuning parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.run_experiment == \u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trainer = SFTTrainer(\u001b[37m\u001b[39;49;00m\n",
      "            model=model,\u001b[37m\u001b[39;49;00m\n",
      "            train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            eval_dataset=eval_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            peft_config=lora_config,\u001b[37m\u001b[39;49;00m\n",
      "            dataset_text_field=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            max_seq_length=args.max_seq_length,\u001b[37m\u001b[39;49;00m\n",
      "            tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "            args=training_arguments,\u001b[37m\u001b[39;49;00m\n",
      "            packing=packing,\u001b[37m\u001b[39;49;00m\n",
      "            callbacks=[SageMakerExperimentsCallback(region=args.region)]\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trainer = SFTTrainer(\u001b[37m\u001b[39;49;00m\n",
      "            model=model,\u001b[37m\u001b[39;49;00m\n",
      "            train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            eval_dataset=eval_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            peft_config=lora_config,\u001b[37m\u001b[39;49;00m\n",
      "            dataset_text_field=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            max_seq_length=args.max_seq_length,\u001b[37m\u001b[39;49;00m\n",
      "            tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "            args=training_arguments,\u001b[37m\u001b[39;49;00m\n",
      "            packing=packing\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    trainer.train()\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_metrics = trainer.evaluate()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrainer.state.log_history\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    model_evaluation(args, trainer.state.log_history)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.merge_weights:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msaving adapter weight combined with the base model weight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# merge adapter weights with base model and save\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# save int 4 model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        new_model = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/lora-adapter-weights\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        trainer.model.save_pretrained(new_model, safe_serialization=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# clear memory\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mdel\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mdel\u001b[39;49;00m trainer\u001b[37m\u001b[39;49;00m\n",
      "        torch.cuda.empty_cache()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        base_model = AutoModelForCausalLM.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "            model_name,\u001b[37m\u001b[39;49;00m\n",
      "            low_cpu_mem_usage=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            return_dict=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            torch_dtype=torch.float16,\u001b[37m\u001b[39;49;00m\n",
      "            device_map=device_map,\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        model = PeftModel.from_pretrained(base_model, new_model)\u001b[37m\u001b[39;49;00m\n",
      "        model = model.merge_and_unload()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        model.save_pretrained(output_dir, safe_serialization=\u001b[34mTrue\u001b[39;49;00m, max_shard_size=\u001b[33m\"\u001b[39;49;00m\u001b[33m2GB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Reload tokenizer to save it\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.pad_token = tokenizer.eos_token\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.padding_side = \u001b[33m\"\u001b[39;49;00m\u001b[33mright\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.save_pretrained(output_dir)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        source_dir = \u001b[33m'\u001b[39;49;00m\u001b[33m./djl-inference/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# copy djl-inference files to model directory\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m os.listdir(source_dir):\u001b[37m\u001b[39;49;00m\n",
      "            source_f = os.path.join(source_dir, f)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# Copy the files to the destination folder\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            shutil.copy(source_f, output_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msaving adapter weights only\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.model.save_pretrained(output_dir, safe_serialization=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parse_arge()\u001b[37m\u001b[39;49;00m\n",
      "    training_function(args)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    main()\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057ab0a-73e0-4d72-ad3f-d47c1f404f97",
   "metadata": {},
   "source": [
    "# Setting up Hyper Parameters for the fine tuning job\n",
    "The following section setup the hyperparameters required for finetuning a QLoRA model. \n",
    "\n",
    "For learn more about the hyperparameter setting for quantization and PEFT, please refer to [this](https://huggingface.co/docs/transformers/main_classes/quantization) and [this](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py) links.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6492f5-abf7-46c5-8912-48f048d82d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker experiment name: exp-NousResearch-Llama-2-7b-chat-hf\n",
      "SageMaker experiment run name: qlora-finetune-run-2408041114-265a3\n",
      "SageMaker training job name: huggingface-qlora-2024-08-04-11-14-24-265a3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "experiments_name = f\"exp-{model_id.replace('/', '-')}\"\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}-{rand_id}\"\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}-{rand_id}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'epochs': 2,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 8,                    # Batch size per GPU for training\n",
    "  'per_device_eval_batch_size' : 8,                    # Batch size per GPU for evaluation\n",
    "  'learning_rate' : 2e-4,                              # Initial learning rate (AdamW optimizer)\n",
    "  'optimizer' : \"paged_adamw_32bit\",                   # Optimizer to use\n",
    "  'logging_steps' : 5,                                 # Log every X updates steps\n",
    "  'lora_r': 64,                                        # LoRA attention dimension.\n",
    "  'lora_alpha' : 16,                                   # The alpha parameter for Lora scaling\n",
    "  'lora_dropout' : 0.1,                                # The dropout probability for Lora layers\n",
    "  'use_4bit' : True,                                   # Activate 4-bit precision base model loading\n",
    "  'bnb_4bit_compute_dtype' : \"float16\",                # Compute dtype for 4-bit base models\n",
    "  'bnb_4bit_quant_type' : \"nf4\",                       # Quantization type (fp4 or nf4)\n",
    "  'base_model_group_name' : base_model_pkg_group_name, # Base model registered in SageMaker Model Registry\n",
    "  'region': region,                                    # AWS region where the training is run\n",
    "  'model_eval_s3_loc' : model_eval_s3_loc              # S3 location for uploading the model evaluation metrics\n",
    "}\n",
    "\n",
    "print(f\"SageMaker experiment name: {experiments_name}\")\n",
    "print(f\"SageMaker experiment run name: {run_name}\")\n",
    "print(f\"SageMaker training job name: {job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a2194-f849-4083-818b-365f552e7387",
   "metadata": {},
   "source": [
    "## Run a SageMaker Training Job\n",
    "In this lab, we'll leverage SageMaker Training job to finetune a Llama2-7b model. The training job includes the following information:\n",
    "\n",
    "* The URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n",
    "* The compute resources that you want SageMaker to use for model training. Compute resources are machine learning (ML) compute instances that are managed by SageMaker.\n",
    "* The URL of the S3 bucket where you want to store the output of the job.\n",
    "* The Amazon Elastic Container Registry path where the training code is stored. For more information.\n",
    "\n",
    "In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at /opt/ml/input/data. Then, it starts the training job by running.\n",
    "\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace Estimator`. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In addition, the Estimator manages the infrastructure use. SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`.\n",
    "\n",
    "After the training job, we'll use the estimator object to deploy the model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96027aea-4865-497a-9f21-3d9079b66b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2024-08-04-11-14-24-2-2024-08-04-11-15-10-527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-04 11:15:11 Starting - Starting the training job\n",
      "2024-08-04 11:15:11 Pending - Training job waiting for capacity..........................................\n",
      "2024-08-04 11:21:54 Pending - Preparing the instances for training...\n",
      "2024-08-04 11:22:48 Downloading - Downloading the training image........................\n",
      "2024-08-04 11:26:35 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-08-04 11:26:53,899 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-08-04 11:26:53,916 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-04 11:26:53,926 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-08-04 11:26:53,928 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-08-04 11:26:55,338 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.16.1)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.9/116.9 kB 7.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.4.7 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.4.7-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.40.2 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.34.34)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2.172.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (0.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (0.13.3)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers==4.31.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.60.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (2.27.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (3.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 8)) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.35.0,>=1.34.34 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 9)) (1.34.34)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 9)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 9)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (2.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (6.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (4.18.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 10)) (1.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.34->boto3->-r requirements.txt (line 9)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.34->boto3->-r requirements.txt (line 9)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (5.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 8)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 2)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 10)) (3.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 8)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 10)) (2023.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 10)) (0.29.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 10)) (0.8.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 10)) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 10)) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker->-r requirements.txt (line 10)) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 8)) (3.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 66.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 23.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 9.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.4.7-py3-none-any.whl (77 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.4/77.4 kB 10.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 13.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 47.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: bitsandbytes, safetensors, transformers, accelerate, peft, trl\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.2 peft-0.4.0 safetensors-0.4.3 transformers-4.31.0 trl-0.4.7\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-08-04 11:27:08,074 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-08-04 11:27:08,074 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-08-04 11:27:08,116 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-04 11:27:08,144 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-04 11:27:08,172 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-08-04 11:27:08,182 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"base_model_group_name\": \"arn:aws:sagemaker:us-east-1:866824485776:model-package-group/NousResearch-Llama-2-7b-chat-hf\",\n",
      "        \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "        \"bnb_4bit_quant_type\": \"nf4\",\n",
      "        \"epochs\": 2,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 5,\n",
      "        \"lora_alpha\": 16,\n",
      "        \"lora_dropout\": 0.1,\n",
      "        \"lora_r\": 64,\n",
      "        \"model_eval_s3_loc\": \"s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/modeleval\",\n",
      "        \"model_id\": \"NousResearch/Llama-2-7b-chat-hf\",\n",
      "        \"optimizer\": \"paged_adamw_32bit\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"per_device_train_batch_size\": 8,\n",
      "        \"region\": \"us-east-1\",\n",
      "        \"use_4bit\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-2024-08-04-11-14-24-2-2024-08-04-11-15-10-527\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-866824485776/huggingface-qlora-2024-08-04-11-14-24-2-2024-08-04-11-15-10-527/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"base_model_group_name\":\"arn:aws:sagemaker:us-east-1:866824485776:model-package-group/NousResearch-Llama-2-7b-chat-hf\",\"bnb_4bit_compute_dtype\":\"float16\",\"bnb_4bit_quant_type\":\"nf4\",\"epochs\":2,\"learning_rate\":0.0002,\"logging_steps\":5,\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":64,\"model_eval_s3_loc\":\"s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/modeleval\",\"model_id\":\"NousResearch/Llama-2-7b-chat-hf\",\"optimizer\":\"paged_adamw_32bit\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"region\":\"us-east-1\",\"use_4bit\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-866824485776/huggingface-qlora-2024-08-04-11-14-24-2-2024-08-04-11-15-10-527/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"base_model_group_name\":\"arn:aws:sagemaker:us-east-1:866824485776:model-package-group/NousResearch-Llama-2-7b-chat-hf\",\"bnb_4bit_compute_dtype\":\"float16\",\"bnb_4bit_quant_type\":\"nf4\",\"epochs\":2,\"learning_rate\":0.0002,\"logging_steps\":5,\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":64,\"model_eval_s3_loc\":\"s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/modeleval\",\"model_id\":\"NousResearch/Llama-2-7b-chat-hf\",\"optimizer\":\"paged_adamw_32bit\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"region\":\"us-east-1\",\"use_4bit\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-2024-08-04-11-14-24-2-2024-08-04-11-15-10-527\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-866824485776/huggingface-qlora-2024-08-04-11-14-24-2-2024-08-04-11-15-10-527/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--base_model_group_name\",\"arn:aws:sagemaker:us-east-1:866824485776:model-package-group/NousResearch-Llama-2-7b-chat-hf\",\"--bnb_4bit_compute_dtype\",\"float16\",\"--bnb_4bit_quant_type\",\"nf4\",\"--epochs\",\"2\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"5\",\"--lora_alpha\",\"16\",\"--lora_dropout\",\"0.1\",\"--lora_r\",\"64\",\"--model_eval_s3_loc\",\"s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/modeleval\",\"--model_id\",\"NousResearch/Llama-2-7b-chat-hf\",\"--optimizer\",\"paged_adamw_32bit\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"8\",\"--region\",\"us-east-1\",\"--use_4bit\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_BASE_MODEL_GROUP_NAME=arn:aws:sagemaker:us-east-1:866824485776:model-package-group/NousResearch-Llama-2-7b-chat-hf\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_COMPUTE_DTYPE=float16\u001b[0m\n",
      "\u001b[34mSM_HP_BNB_4BIT_QUANT_TYPE=nf4\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=5\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=16\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_EVAL_S3_LOC=s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/modeleval\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=NousResearch/Llama-2-7b-chat-hf\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=paged_adamw_32bit\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=us-east-1\u001b[0m\n",
      "\u001b[34mSM_HP_USE_4BIT=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --base_model_group_name arn:aws:sagemaker:us-east-1:866824485776:model-package-group/NousResearch-Llama-2-7b-chat-hf --bnb_4bit_compute_dtype float16 --bnb_4bit_quant_type nf4 --epochs 2 --learning_rate 0.0002 --logging_steps 5 --lora_alpha 16 --lora_dropout 0.1 --lora_r 64 --model_eval_s3_loc s3://sagemaker-us-east-1-866824485776/data/workshop-265a3/modeleval --model_id NousResearch/Llama-2-7b-chat-hf --optimizer paged_adamw_32bit --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --region us-east-1 --use_4bit True\u001b[0m\n",
      "\u001b[34m2024-08-04 11:27:08,213 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mmerging weights: True\u001b[0m\n",
      "\u001b[34mfound model package: arn:aws:sagemaker:us-east-1:866824485776:model-package/NousResearch-Llama-2-7b-chat-hf/1\u001b[0m\n",
      "\u001b[34mDownloading file from sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz to /tmp/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m0.0%|                          | ?B/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m0.0%|                          | 2.16MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m0.2%|                          | 131MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m0.2%|                          | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m0.5%|▏                         | 213MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m0.8%|▏                         | 237MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m1.1%|▎                         | 254MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m1.5%|▎                         | 305MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m1.8%|▍                         | 313MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m1.8%|▍                         | 319MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m2.1%|▌                         | 304MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m2.4%|▌                         | 294MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m2.7%|▋                         | 296MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m3.1%|▊                         | 325MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m3.4%|▊                         | 325MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m3.7%|▉                         | 333MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m4.0%|█                         | 302MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m4.3%|█                         | 296MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m4.4%|█                         | 291MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m4.6%|█▏                        | 293MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m4.9%|█▏                        | 295MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m5.3%|█▎                        | 313MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m5.6%|█▍                        | 307MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m5.9%|█▍                        | 305MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m6.2%|█▌                        | 322MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m6.2%|█▌                        | 335MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m6.6%|█▋                        | 331MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m6.9%|█▋                        | 342MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m7.3%|█▊                        | 342MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m7.7%|█▉                        | 364MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m8.0%|██                        | 334MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m8.4%|██                        | 343MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m8.7%|██▏                       | 331MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m9.1%|██▎                       | 343MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m9.4%|██▎                       | 335MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m9.7%|██▍                       | 326MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m10.1%|██▌                       | 309MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m10.4%|██▌                       | 310MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m10.7%|██▋                       | 293MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m10.7%|██▋                       | 283MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m10.9%|██▋                       | 276MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m10.9%|██▋                       | 271MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m11.2%|██▊                       | 277MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m11.5%|██▊                       | 277MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m11.8%|██▉                       | 276MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m11.8%|██▉                       | 276MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m12.1%|███                       | 283MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m12.3%|███                       | 276MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m12.6%|███▏                      | 277MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m12.6%|███▏                      | 277MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m12.9%|███▏                      | 296MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m13.3%|███▎                      | 325MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m13.3%|███▎                      | 346MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m13.7%|███▍                      | 339MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m14.0%|███▍                      | 223MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m14.3%|███▌                      | 246MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m14.6%|███▋                      | 260MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m14.9%|███▋                      | 269MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m15.2%|███▊                      | 273MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m15.5%|███▉                      | 299MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m16.0%|████                      | 354MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m16.4%|████                      | 318MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m16.7%|████▏                     | 315MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m17.0%|████▎                     | 323MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m17.4%|████▎                     | 315MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m17.7%|████▍                     | 306MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m18.0%|████▍                     | 312MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m18.3%|████▌                     | 308MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m18.6%|████▋                     | 295MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m18.9%|████▋                     | 295MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m19.2%|████▊                     | 287MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m19.5%|████▊                     | 288MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m19.8%|████▉                     | 309MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m20.1%|█████                     | 312MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m20.5%|█████                     | 334MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m20.8%|█████▏                    | 313MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m21.1%|█████▎                    | 308MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m21.4%|█████▎                    | 306MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m21.7%|█████▍                    | 311MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m22.1%|█████▌                    | 312MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m22.4%|█████▌                    | 295MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m22.7%|█████▋                    | 320MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m23.0%|█████▊                    | 314MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m23.3%|█████▊                    | 291MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m23.7%|█████▉                    | 316MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m24.0%|██████                    | 308MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m24.3%|██████                    | 279MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m24.6%|██████▏                   | 289MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m24.9%|██████▏                   | 286MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m25.3%|██████▎                   | 304MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m25.6%|██████▍                   | 315MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m25.9%|██████▍                   | 313MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m26.2%|██████▌                   | 298MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m26.5%|██████▋                   | 292MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m26.8%|██████▋                   | 296MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m27.1%|██████▊                   | 307MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m27.1%|██████▊                   | 316MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m27.4%|██████▊                   | 303MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m27.7%|██████▉                   | 304MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m28.0%|███████                   | 295MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m28.3%|███████                   | 292MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m28.3%|███████                   | 291MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m28.3%|███████                   | 291MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m28.6%|███████▏                  | 287MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m29.0%|███████▏                  | 295MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m29.3%|███████▎                  | 295MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m29.6%|███████▍                  | 293MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m29.8%|███████▍                  | 291MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m30.1%|███████▌                  | 287MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m30.4%|███████▌                  | 285MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m30.7%|███████▋                  | 281MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m31.0%|███████▋                  | 292MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m31.3%|███████▊                  | 280MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m31.6%|███████▉                  | 293MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m31.9%|███████▉                  | 287MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m31.9%|███████▉                  | 284MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m32.2%|████████                  | 283MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m32.5%|████████                  | 290MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m32.7%|████████▏                 | 292MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m33.2%|████████▎                 | 336MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m33.5%|████████▎                 | 336MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m33.8%|████████▍                 | 342MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m34.2%|████████▌                 | 322MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m34.5%|████████▌                 | 300MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m34.5%|████████▋                 | 286MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m34.8%|████████▋                 | 292MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m35.2%|████████▊                 | 325MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m35.2%|████████▊                 | 350MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m35.6%|████████▉                 | 367MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m36.0%|████████▉                 | 345MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m36.3%|█████████                 | 356MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m36.7%|█████████▏                | 331MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m37.0%|█████████▎                | 334MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m37.3%|█████████▎                | 333MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m37.7%|█████████▍                | 313MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m37.7%|█████████▍                | 301MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m38.0%|█████████▍                | 299MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m38.3%|█████████▌                | 303MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m38.6%|█████████▋                | 302MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m38.9%|█████████▋                | 304MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m38.9%|█████████▋                | 306MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m39.2%|█████████▊                | 297MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m39.5%|█████████▊                | 294MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m39.8%|█████████▉                | 300MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m40.2%|██████████                | 326MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m40.2%|██████████                | 345MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m40.6%|██████████▏               | 371MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m40.9%|██████████▏               | 350MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m41.3%|██████████▎               | 349MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m41.6%|██████████▍               | 330MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m42.0%|██████████▍               | 344MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m42.4%|██████████▌               | 359MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m42.7%|██████████▋               | 329MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m43.0%|██████████▊               | 319MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m43.4%|██████████▊               | 311MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m43.4%|██████████▊               | 305MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m43.7%|██████████▉               | 310MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m44.0%|██████████▉               | 313MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m44.4%|███████████               | 343MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m44.7%|███████████▏              | 282MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m44.7%|███████████▏              | 251MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m45.1%|███████████▎              | 279MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m45.4%|███████████▎              | 284MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m45.8%|███████████▍              | 315MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m46.1%|███████████▌              | 316MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m46.4%|███████████▌              | 308MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m46.4%|███████████▌              | 303MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m46.8%|███████████▋              | 323MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m47.1%|███████████▊              | 316MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m47.4%|███████████▊              | 309MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m47.7%|███████████▉              | 311MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m48.1%|████████████              | 317MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m48.4%|████████████              | 308MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m48.7%|████████████▏             | 291MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m49.0%|████████████▏             | 290MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m49.3%|████████████▎             | 297MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m49.3%|████████████▎             | 303MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m49.6%|████████████▍             | 315MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m49.6%|████████████▍             | 323MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m49.9%|████████████▍             | 304MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m50.2%|████████████▌             | 304MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m50.2%|████████████▌             | 305MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m50.6%|████████████▋             | 332MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m50.9%|████████████▋             | 333MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m51.3%|████████████▊             | 346MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m51.6%|████████████▉             | 296MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m51.9%|████████████▉             | 301MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m52.2%|█████████████             | 304MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m52.6%|█████████████▏            | 329MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m53.0%|█████████████▏            | 311MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m53.0%|█████████████▏            | 299MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m53.3%|█████████████▎            | 327MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m53.7%|█████████████▍            | 311MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m54.0%|█████████████▍            | 312MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m54.3%|█████████████▌            | 318MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m54.6%|█████████████▋            | 306MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m54.9%|█████████████▋            | 303MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m55.2%|█████████████▊            | 309MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m55.2%|█████████████▊            | 313MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m55.5%|█████████████▉            | 308MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m55.8%|█████████████▉            | 301MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m56.1%|██████████████            | 280MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m56.4%|██████████████            | 284MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m56.7%|██████████████▏           | 289MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m57.0%|██████████████▎           | 298MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m57.4%|██████████████▎           | 314MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m57.7%|██████████████▍           | 301MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m58.0%|██████████████▍           | 304MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m58.3%|██████████████▌           | 303MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m58.3%|██████████████▌           | 303MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m58.6%|██████████████▋           | 321MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m59.0%|██████████████▋           | 336MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m59.4%|██████████████▊           | 370MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m59.8%|██████████████▉           | 323MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m60.2%|███████████████           | 343MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m60.5%|███████████████▏          | 343MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m60.9%|███████████████▏          | 337MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m60.9%|███████████████▏          | 334MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m61.2%|███████████████▎          | 324MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m61.5%|███████████████▎          | 314MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m61.8%|███████████████▍          | 307MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m62.1%|███████████████▌          | 316MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m62.4%|███████████████▌          | 308MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m62.4%|███████████████▌          | 301MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m62.4%|███████████████▌          | 301MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m62.7%|███████████████▋          | 293MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m63.1%|███████████████▊          | 307MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m63.5%|███████████████▊          | 327MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m63.5%|███████████████▊          | 342MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m63.8%|███████████████▉          | 357MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m64.2%|████████████████          | 340MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m64.5%|████████████████▏         | 347MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m64.9%|████████████████▏         | 360MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m65.3%|████████████████▎         | 356MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m65.7%|████████████████▍         | 333MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m66.1%|████████████████▌         | 325MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m66.4%|████████████████▌         | 334MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m66.4%|████████████████▌         | 342MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m66.8%|████████████████▋         | 336MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m67.1%|████████████████▊         | 316MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m67.4%|████████████████▊         | 321MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m67.7%|████████████████▉         | 320MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m68.1%|█████████████████         | 329MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m68.4%|█████████████████         | 321MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m68.7%|█████████████████▏        | 310MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m69.0%|█████████████████▎        | 285MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m69.3%|█████████████████▎        | 259MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m69.5%|█████████████████▍        | 238MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m69.8%|█████████████████▍        | 233MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m70.0%|█████████████████▌        | 225MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m70.2%|█████████████████▌        | 223MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m70.4%|█████████████████▌        | 214MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m70.7%|█████████████████▋        | 216MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m70.9%|█████████████████▋        | 222MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m71.1%|█████████████████▊        | 218MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m71.3%|█████████████████▊        | 224MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m71.3%|█████████████████▊        | 228MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m71.6%|█████████████████▉        | 227MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m71.8%|█████████████████▉        | 223MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m71.8%|█████████████████▉        | 221MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m72.0%|██████████████████        | 221MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m72.2%|██████████████████        | 221MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m72.4%|██████████████████        | 218MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m72.7%|██████████████████▏       | 222MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m72.9%|██████████████████▏       | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m72.9%|██████████████████▏       | 156MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m73.2%|██████████████████▎       | 185MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m73.4%|██████████████████▎       | 215MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m73.7%|██████████████████▍       | 230MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m74.0%|██████████████████▍       | 225MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m74.2%|██████████████████▌       | 223MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m74.4%|██████████████████▌       | 225MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m74.6%|██████████████████▋       | 225MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m74.9%|██████████████████▋       | 223MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m75.1%|██████████████████▊       | 219MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m75.1%|██████████████████▊       | 216MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m75.3%|██████████████████▊       | 219MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m75.5%|██████████████████▉       | 219MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m75.7%|██████████████████▉       | 219MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m76.0%|██████████████████▉       | 220MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m76.2%|███████████████████       | 219MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m76.4%|███████████████████       | 218MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m76.6%|███████████████████▏      | 221MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m76.9%|███████████████████▏      | 221MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m77.1%|███████████████████▎      | 223MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m77.3%|███████████████████▎      | 223MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m77.5%|███████████████████▍      | 224MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m77.7%|███████████████████▍      | 222MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m78.0%|███████████████████▍      | 218MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m78.2%|███████████████████▌      | 224MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m78.4%|███████████████████▌      | 221MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m78.6%|███████████████████▋      | 196MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m78.8%|███████████████████▋      | 193MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m78.8%|███████████████████▋      | 192MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m78.8%|███████████████████▋      | 192MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m79.0%|███████████████████▊      | 183MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m79.2%|███████████████████▊      | 183MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m79.4%|███████████████████▊      | 184MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m79.6%|███████████████████▉      | 185MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m79.8%|███████████████████▉      | 188MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m80.0%|███████████████████▉      | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m80.1%|████████████████████      | 175MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m80.3%|████████████████████      | 177MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m80.5%|████████████████████      | 176MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m80.5%|████████████████████      | 175MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m80.7%|████████████████████▏     | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m80.7%|████████████████████▏     | 182MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m80.9%|████████████████████▏     | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m81.0%|████████████████████▎     | 182MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m81.2%|████████████████████▎     | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m81.4%|████████████████████▎     | 173MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m81.6%|████████████████████▍     | 174MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m81.8%|████████████████████▍     | 177MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m81.8%|████████████████████▍     | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m81.9%|████████████████████▍     | 173MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m82.1%|████████████████████▌     | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m82.3%|████████████████████▌     | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m82.5%|████████████████████▌     | 161MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m82.7%|████████████████████▋     | 171MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m82.9%|████████████████████▋     | 127MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m83.2%|████████████████████▊     | 168MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m83.5%|████████████████████▊     | 206MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m83.7%|████████████████████▉     | 193MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m83.9%|████████████████████▉     | 186MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m84.1%|█████████████████████     | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m84.3%|█████████████████████     | 175MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m84.5%|█████████████████████     | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m84.5%|█████████████████████     | 170MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m84.7%|█████████████████████▏    | 170MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m84.8%|█████████████████████▏    | 170MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m84.8%|█████████████████████▏    | 171MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m85.0%|█████████████████████▎    | 170MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m85.2%|█████████████████████▎    | 174MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m85.4%|█████████████████████▎    | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m85.4%|█████████████████████▎    | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m85.4%|█████████████████████▎    | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m85.5%|█████████████████████▍    | 170MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m85.7%|█████████████████████▍    | 164MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m85.9%|█████████████████████▍    | 173MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.1%|█████████████████████▌    | 171MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.2%|█████████████████████▌    | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.4%|█████████████████████▌    | 166MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.4%|█████████████████████▌    | 161MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.4%|█████████████████████▌    | 161MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.4%|█████████████████████▌    | 161MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.6%|█████████████████████▋    | 155MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.8%|█████████████████████▋    | 162MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.8%|█████████████████████▋    | 169MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m86.9%|█████████████████████▋    | 170MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m87.1%|█████████████████████▊    | 170MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m87.3%|█████████████████████▊    | 169MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m87.5%|█████████████████████▊    | 174MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m87.6%|█████████████████████▉    | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m87.8%|█████████████████████▉    | 168MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m88.0%|█████████████████████▉    | 168MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m88.1%|██████████████████████    | 166MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m88.3%|██████████████████████    | 166MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m88.5%|██████████████████████    | 169MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m88.6%|██████████████████████▏   | 166MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m88.8%|██████████████████████▏   | 169MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m88.8%|██████████████████████▏   | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m89.0%|██████████████████████▏   | 167MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m89.0%|██████████████████████▏   | 164MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m89.2%|██████████████████████▎   | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m89.3%|██████████████████████▎   | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m89.5%|██████████████████████▍   | 173MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m89.7%|██████████████████████▍   | 171MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m89.9%|██████████████████████▍   | 172MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m90.0%|██████████████████████▌   | 175MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m90.2%|██████████████████████▌   | 174MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m90.4%|██████████████████████▌   | 171MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m90.6%|██████████████████████▋   | 171MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m90.7%|██████████████████████▋   | 150MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m91.0%|██████████████████████▊   | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m91.2%|██████████████████████▊   | 180MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m91.4%|██████████████████████▊   | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m91.6%|██████████████████████▉   | 177MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m91.8%|██████████████████████▉   | 181MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m92.0%|██████████████████████▉   | 181MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m92.1%|███████████████████████   | 180MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m92.1%|███████████████████████   | 180MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m92.3%|███████████████████████   | 171MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m92.5%|███████████████████████   | 171MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m92.7%|███████████████████████▏  | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m92.8%|███████████████████████▏  | 176MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.0%|███████████████████████▎  | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.2%|███████████████████████▎  | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.2%|███████████████████████▎  | 181MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.2%|███████████████████████▎  | 181MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.4%|███████████████████████▎  | 179MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.4%|███████████████████████▎  | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.4%|███████████████████████▎  | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.6%|███████████████████████▍  | 174MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.7%|███████████████████████▍  | 173MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.9%|███████████████████████▍  | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.9%|███████████████████████▍  | 185MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m93.9%|███████████████████████▍  | 185MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m94.1%|███████████████████████▌  | 180MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m94.3%|███████████████████████▌  | 174MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m94.5%|███████████████████████▌  | 178MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m94.7%|███████████████████████▋  | 175MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m94.8%|███████████████████████▋  | 175MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m95.1%|███████████████████████▊  | 200MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m95.3%|███████████████████████▊  | 215MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m95.6%|███████████████████████▉  | 233MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m95.9%|███████████████████████▉  | 244MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m96.1%|████████████████████████  | 245MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m96.4%|████████████████████████  | 250MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m96.6%|████████████████████████▏ | 259MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m96.9%|████████████████████████▏ | 259MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m97.1%|████████████████████████▎ | 260MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m97.4%|████████████████████████▎ | 256MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m97.6%|████████████████████████▍ | 256MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m97.9%|████████████████████████▍ | 261MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m98.2%|████████████████████████▌ | 264MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m98.4%|████████████████████████▌ | 262MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m98.7%|████████████████████████▋ | 261MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m98.7%|████████████████████████▋ | 261MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m99.0%|████████████████████████▋ | 263MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m99.2%|████████████████████████▊ | 260MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m99.2%|████████████████████████▊ | 259MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m99.5%|████████████████████████▊ | 259MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m99.7%|████████████████████████▉ | 260MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m100.0%|████████████████████████▉ | 259MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34m100.0%|█████████████████████████ | 262MB/s | source: s3://sagemaker-us-east-1-866824485776/data/NousResearch/Llama-2-7b-chat-hf/basemodel/model.tar.gz\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  4.74s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.60s/it]\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mYour GPU supports bfloat16: accelerate training with bf16=True\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m[sm-callback] loaded sagemaker Experiment (name: exp-nousresearch-llama-2-7b-chat-hf) with run: qlora-finetune-run-2408041114-265a3!\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/49 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 49/49 [00:00<00:00, 669.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/49 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 49/49 [00:00<00:00, 677.99 examples/s]\u001b[0m\n",
      "\u001b[34m[sm-callback] adding parameters to exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] TrainerState(epoch=None, global_step=0, max_steps=0, num_train_epochs=0, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': None, 'lr_scheduler': None, 'train_dataloader': None, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m0%|          | 0/14 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0, global_step=0, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.14285714285714285, global_step=1, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m7%|▋         | 1/14 [00:14<03:07, 14.45s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.14285714285714285, global_step=1, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.2857142857142857, global_step=2, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 2/14 [00:27<02:43, 13.64s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.2857142857142857, global_step=2, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.42857142857142855, global_step=3, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 3/14 [00:40<02:27, 13.39s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.42857142857142855, global_step=3, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.5714285714285714, global_step=4, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 4/14 [00:53<02:12, 13.26s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.5714285714285714, global_step=4, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.7142857142857143, global_step=5, max_steps=14, num_train_epochs=2, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=True, should_log=True)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': None}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 5/14 [01:06<01:58, 13.20s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('loss', 1.7948), ('learning_rate', 0.0002), ('epoch', 0.71)])\u001b[0m\n",
      "\u001b[34m{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 5/14 [01:07<01:58, 13.20s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.95s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:11,  2.77s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:11<00:09,  3.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:15<00:06,  3.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:19<00:03,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:20<00:00,  2.90s/it]#033[A\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('eval_loss', 1.6201680898666382), ('eval_runtime', 24.0884), ('eval_samples_per_second', 2.034), ('eval_steps_per_second', 0.291), ('epoch', 0.71)])\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 5/14 [01:32<01:58, 13.20s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:21<00:00,  2.90s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[sm-callback] on_evaluation is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.7142857142857143, global_step=5, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.7142857142857143, global_step=5, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.8571428571428571, global_step=6, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 6/14 [01:46<02:57, 22.17s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=0.8571428571428571, global_step=6, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.0, global_step=7, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m50%|█████     | 7/14 [01:48<01:49, 15.61s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_epoch_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.0, global_step=7, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] start: 0 ep to end: 1.0 ep!\u001b[0m\n",
      "\u001b[34mepoch_loss_values: {0.71: 1.7948}\u001b[0m\n",
      "\u001b[34mepoch_eval_loss_values: {0.71: 1.6201680898666382}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.0, global_step=7, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.1428571428571428, global_step=8, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 8/14 [02:02<01:30, 15.04s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.1428571428571428, global_step=8, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.2857142857142856, global_step=9, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 9/14 [02:15<01:12, 14.43s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.2857142857142856, global_step=9, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.4285714285714286, global_step=10, max_steps=14, num_train_epochs=2, total_flos=867348177223680.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=True, should_log=True)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15fd8490>}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 10/14 [02:28<00:56, 14.01s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('loss', 1.4992), ('learning_rate', 0.0002), ('epoch', 1.43)])\u001b[0m\n",
      "\u001b[34m{'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 10/14 [02:29<00:56, 14.01s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.95s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:11,  2.77s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:11<00:09,  3.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:15<00:06,  3.44s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:19<00:03,  3.59s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:20<00:00,  2.89s/it]#033[A\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('eval_loss', 1.5246349573135376), ('eval_runtime', 24.0691), ('eval_samples_per_second', 2.036), ('eval_steps_per_second', 0.291), ('epoch', 1.43)])\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 10/14 [02:54<00:56, 14.01s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:21<00:00,  2.89s/it]#033[A\u001b[0m\n",
      "\u001b[34m[sm-callback] on_evaluation is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.4285714285714286, global_step=10, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.4285714285714286, global_step=10, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15bdbf70>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.5714285714285714, global_step=11, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15bdbf70>}\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 11/14 [03:07<01:05, 21.83s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.5714285714285714, global_step=11, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15bdbf70>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.7142857142857144, global_step=12, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15bdbf70>}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 12/14 [03:21<00:38, 19.17s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.7142857142857144, global_step=12, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15bdbf70>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.8571428571428572, global_step=13, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15bdbf70>}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 13/14 [03:34<00:17, 17.32s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_begin is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=1.8571428571428572, global_step=13, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15bdbf70>}\u001b[0m\n",
      "\u001b[34m[sm-callback] on_step_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=2.0, global_step=14, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] control: TrainerControl(should_training_stop=True, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)\u001b[0m\n",
      "\u001b[34m[sm-callback] kwargs: {'model': PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=11008, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=11008, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\u001b[0m\n",
      "\u001b[34m), 'tokenizer': LlamaTokenizerFast(name_or_path='/tmp/basemodel', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '</s>'}, clean_up_tokenization_spaces=False), 'optimizer': AcceleratedOptimizer (\u001b[0m\n",
      "\u001b[34mParameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.001\u001b[0m\n",
      "\u001b[34mParameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.0002\n",
      "    lr: 0.0002\n",
      "    weight_decay: 0.0\u001b[0m\n",
      "\u001b[34m), 'lr_scheduler': <torch.optim.lr_scheduler.LambdaLR object at 0x7f0a148d8940>, 'train_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a16ce9ae0>, 'eval_dataloader': <accelerate.data_loader.DataLoaderShard object at 0x7f0a15bdbf70>}\u001b[0m\n",
      "\u001b[34m100%|██████████| 14/14 [03:36<00:00, 12.72s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] on_epoch_end is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=2.0, global_step=14, max_steps=14, num_train_epochs=2, total_flos=1582910423433216.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m[sm-callback] start: 1.0 ep to end: 2.0 ep!\u001b[0m\n",
      "\u001b[34mepoch_loss_values: {1.43: 1.4992}\u001b[0m\n",
      "\u001b[34mepoch_eval_loss_values: {1.43: 1.5246349573135376}\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('train_runtime', 217.1235), ('train_samples_per_second', 0.451), ('train_steps_per_second', 0.064), ('total_flos', 2125003034198016.0), ('train_loss', 1.5750625814710344), ('epoch', 2.0)])\u001b[0m\n",
      "\u001b[34m{'train_runtime': 217.1235, 'train_samples_per_second': 0.451, 'train_steps_per_second': 0.064, 'train_loss': 1.5750625814710344, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 14/14 [03:37<00:00, 12.72s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 14/14 [03:37<00:00, 15.57s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:09,  1.95s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:11,  2.77s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:11<00:09,  3.19s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:15<00:06,  3.44s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:19<00:03,  3.59s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:20<00:00,  2.89s/it]\u001b[0m\n",
      "\u001b[34m[sm-callback] logging is called exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] logging items: dict_items([('eval_loss', 1.4996049404144287), ('eval_runtime', 24.0692), ('eval_samples_per_second', 2.036), ('eval_steps_per_second', 0.291), ('epoch', 2.0)])\u001b[0m\n",
      "\u001b[34m[sm-callback] on_evaluation is called: exp-nousresearch-llama-2-7b-chat-hf: qlora-finetune-run-2408041114-265a3\u001b[0m\n",
      "\u001b[34m[sm-callback] state: TrainerState(epoch=2.0, global_step=14, max_steps=14, num_train_epochs=2, total_flos=2125003034198016.0, log_history=[{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}, {'train_runtime': 217.1235, 'train_samples_per_second': 0.451, 'train_steps_per_second': 0.064, 'total_flos': 2125003034198016.0, 'train_loss': 1.5750625814710344, 'epoch': 2.0, 'step': 14}, {'eval_loss': 1.4996049404144287, 'eval_runtime': 24.0692, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 2.0, 'step': 14}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:22<00:00,  3.21s/it]\u001b[0m\n",
      "\u001b[34mTraining metrics: [{'loss': 1.7948, 'learning_rate': 0.0002, 'epoch': 0.71, 'step': 5}, {'eval_loss': 1.6201680898666382, 'eval_runtime': 24.0884, 'eval_samples_per_second': 2.034, 'eval_steps_per_second': 0.291, 'epoch': 0.71, 'step': 5}, {'loss': 1.4992, 'learning_rate': 0.0002, 'epoch': 1.43, 'step': 10}, {'eval_loss': 1.5246349573135376, 'eval_runtime': 24.0691, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 1.43, 'step': 10}, {'train_runtime': 217.1235, 'train_samples_per_second': 0.451, 'train_steps_per_second': 0.064, 'total_flos': 2125003034198016.0, 'train_loss': 1.5750625814710344, 'epoch': 2.0, 'step': 14}, {'eval_loss': 1.4996049404144287, 'eval_runtime': 24.0692, 'eval_samples_per_second': 2.036, 'eval_steps_per_second': 0.291, 'epoch': 2.0, 'step': 14}]\u001b[0m\n",
      "\u001b[34mevaluation metrics: {'regression_metrics': {'train_loss': {'value': 1.5750625814710344}, 'eval_loss': {'value': 1.4996049404144287}}}\u001b[0m\n",
      "\u001b[34msaving adapter weight combined with the base model weight\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]\u001b[0m\n",
      "\u001b[34m2024-08-04 11:38:20,114 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-08-04 11:38:20,114 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-08-04 11:38:20,115 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-08-04 11:38:38 Uploading - Uploading generated training model\n",
      "2024-08-04 11:41:06 Completed - Training job completed\n",
      "Training seconds: 1119\n",
      "Billable seconds: 1119\n"
     ]
    }
   ],
   "source": [
    "with Run(\n",
    "    experiment_name=experiments_name,\n",
    "    run_name=run_name,\n",
    "    sagemaker_session=sess\n",
    ") as run:\n",
    "\n",
    "    # create the Estimator\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',         # train script\n",
    "        source_dir='src/train',         # directory which includes all the files needed for training\n",
    "        instance_type='ml.g5.2xlarge', # instances type used for the training job\n",
    "        # instance_type='local_gpu',      # use local \n",
    "        instance_count=1,               # the number of instances used for training\n",
    "        base_job_name=job_name,         # the name of the training job\n",
    "        role=get_execution_role(),      # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "        volume_size=300,    # the size of the EBS volume in GB\n",
    "        transformers_version='4.28.1',    # the transformers version used in the training job\n",
    "        pytorch_version='2.0.0',          # the pytorch_version version used in the training job\n",
    "        py_version='py310',             # the python version used in the training job\n",
    "        hyperparameters= hyperparameters,\n",
    "        environment={ \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "        sagemaker_session=sess,         # specifies a sagemaker session object\n",
    "        output_path=model_output_s3_loc # s3 location for model artifact,\n",
    "    )\n",
    "    \n",
    "    # define a data input dictonary with our uploaded s3 uris\n",
    "    data = { 'training': training_dataset_s3_loc,\n",
    "             'validation': validation_dataset_s3_loc}\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    huggingface_estimator.fit(data, wait=True)\n",
    "    run.log_parameters(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db322bf5-a425-43d5-a30b-479334777013",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker Experiment Integration\n",
    "### Overview\n",
    "Machine learning is an iterative process. You need to experiment with multiple combinations of data, algorithms, and parameters, all while observing the impact of incremental changes on model accuracy. Over time, this iterative experimentation can result in thousands of model training runs and model versions. This makes it hard to track the best performing models and their input configurations. It’s also difficult to compare active experiments with past experiments to identify opportunities for further incremental improvements. Use SageMaker Experiments to organize, view, analyze, and compare iterative ML experimentation to gain comparative insights and track your best performing models.\n",
    "\n",
    "### SageMaker Experiment\n",
    "Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you create, manage, analyze, and compare your machine learning experiments.\n",
    "SageMaker Experiments automatically tracks the inputs, parameters, configurations, and results of your iterations as runs. You can assign, group, and organize these runs into experiments. SageMaker Experiments is integrated with Amazon SageMaker Studio, providing a visual interface to browse your active and past experiments, compare runs on key performance metrics, and identify the best performing models. SageMaker Experiments tracks all of the steps and artifacts that went into creating a model, and you can quickly revisit the origins of a model when you are troubleshooting issues in production, or auditing your models for compliance verifications.\n",
    "\n",
    "As we saw in the training job configuration above, we enabled SageMaker Experiment to capture the relevant metrics to help us visualize the training and evalution metrics from the training run. To access these metrics in SageMaker experiment, you would navigate from the left panel:\n",
    "click on the Home icon ![home](images/home.png) -> Experiments -> name of the experiment (e.g. exp-nousresearch-llama-2-7b-chat-hf) -> run name.\n",
    "\n",
    "Here's are a screenshot of SageMaker Experiment that captures the metrics from a previous run:\n",
    "<div>\n",
    "<img src=\"images/experiment-metrics.png\" width=\"800\"/>\n",
    "<img src=\"images/exp-metrics-chart.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "For see more examples on SageMaker Experiment, please visit [amazon-sagemake-examples](https://github.com/aws/amazon-sagemaker-examples/tree/main/sagemaker-experiments) github reposistory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c9a15-e812-43f1-997e-1632ba3a5598",
   "metadata": {},
   "source": [
    "# Deploy the finetuned Llama2 model in SageMaker\n",
    "State-of-the-art deep learning models for applications such as natural language processing (NLP) are large, typically with tens or hundreds of billions of parameters. Larger models are often more accurate, which makes them attractive to machine learning practitioners. However, these models are often too large to fit on a single accelerator or GPU device, making it difficult to achieve low-latency inference. You can avoid this memory bottleneck by using model parallelism techniques to partition a model across multiple accelerators or GPUs.\n",
    "\n",
    "Amazon SageMaker includes specialized deep learning containers (DLCs), libraries, and tooling for model parallelism and large model inference (LMI). In the following sections, you can find resources to get started with LMI on SageMaker.\n",
    "\n",
    "With these DLCs you can use third party libraries such as [DeepSpeed](https://github.com/microsoft/DeepSpeed), [Accelerate](https://huggingface.co/docs/accelerate), and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) to partition model parameters using model parallelism techniques to leverage the memory of multiple GPUs for inference.\n",
    "\n",
    "After the training job, we will deploy the QLoRA finetuned model into SageMaker for inference. In our example, we will also use a Large Model Inference(LMI) container provided by AWS using `DJL Serving` and `DeepSpeed`. Given the llama2-7b model size, this model could fit in a single `ml.g5.2xlarge` instance on AWS SageMaker.\n",
    "\n",
    "### Deep Java Library (DJL) \n",
    "Deep Java Library (DJL) Serving is a high performance universal stand-alone model serving solution powered by DJL. DJL Serving supports loading models trained with a variety of different frameworks. With the SageMaker Python SDK you can use DJL Serving to host large models using backends like DeepSpeed and HuggingFace Accelerate.\n",
    "\n",
    "For more information about using `DJL Serving` model server for hosting LLMs in SageMaker, please refer to the following:\n",
    "\n",
    "* [DeepSpeed and Accelerate](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-deepspeed-djl.html)\n",
    "* [FasterTransformer](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-fastertransformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7604071-1db4-4376-90a4-3e398a9d1769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "llm_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e24cf047-f104-46c8-bf8f-5132bfb083f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 3600\n",
    "\n",
    "# create HuggingFaceModel with the a DJI image uri\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=huggingface_estimator.model_data,\n",
    "    image_uri=llm_image,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ffe1d-7e7c-4a08-899b-565331c36b93",
   "metadata": {},
   "source": [
    "Trigger a SageMaker deployment by invoking huggingface model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27bb32f0-2d61-4291-a79b-84fdf0fa2ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: djl-inference-2024-08-04-11-41-17-360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: llama2-7b-djl-deepspeed-baf73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name llama2-7b-djl-deepspeed-baf73\n",
      "INFO:sagemaker:Creating endpoint with name llama2-7b-djl-deepspeed-baf73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name_random_id = uuid.uuid4().hex[:5]\n",
    "endpoint_name = f\"llama2-7b-djl-deepspeed-{endpoint_name_random_id}\"\n",
    "\n",
    "print(f\"endpoint name: {endpoint_name}\")\n",
    "llm = huggingface_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, \n",
    "  endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c38b8-dfa2-44f6-9fae-f96bf35d3883",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test the model\n",
    "In the following section, we'll run a test against the deployed endpoint. Here we format the \n",
    "prompt using the llama2 [standard prompt](https://huggingface.co/blog/llama2#how-to-prompt-llama-2).\n",
    "\n",
    "In the test data, we provide a system prompt along with a question and a few contextual information that might be relevant to the answer. Let's see how the model performs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "205f64c8-76f1-43ea-b61c-e38c12744f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>\n",
    "[INST] <<SYS>>\n",
    "{{system}}\n",
    "<</SYS>>\n",
    "\n",
    "### Question\n",
    "{{question}}\n",
    "\n",
    "### Context\n",
    "{{context}}[/INST] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53dcdb7f-4dcc-42cd-9c93-3133bf19a403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"Given the following context, answer the question as accurately as possible:\"\n",
    "def build_llama2_prompt(message):\n",
    "    question = message['question']\n",
    "    context = message['context']\n",
    "    formatted_message = prompt_template.replace(\"{{system}}\", system_message)\n",
    "    formatted_message = formatted_message.replace(\"{{question}}\", question)\n",
    "    formatted_message = formatted_message.replace(\"{{context}}\", context)\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21659f11-b9ce-44ad-bfb1-20f0a4231d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = {}\n",
    "message['question'] = \"The Oberoi family is part of a hotel company that has a head office in what city?\"\n",
    "message['context'] = \"\"\"The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta. It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two towers that comprises a hotel and the Airlangga Apartment respectively. The hotel was opened in 2005.\n",
    "The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.\n",
    "The Oberoi Group is a hotel company with its head office in Delhi. Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.\n",
    "The 289th Military Police Company was activated on 1 November 1994 and attached to Hotel Company, 3rd Infantry (The Old Guard), Fort Myer, Virginia. Hotel Company is the regiment\\'s specialty company.\\nThe Glennwanis Hotel is a historic hotel in Glennville, Georgia, Tattnall County, Georgia, built on the site of the Hughes Hotel. The hotel is located at 209-215 East Barnard Street. The old Hughes Hotel was built out of Georgia pine circa 1905 and burned in 1920. The Glennwanis was built in brick in 1926. The local Kiwanis club led the effort to get the replacement hotel built, and organized a Glennville Hotel Company with directors being local business leaders. The wife of a local doctor won a naming contest with the name \"Glennwanis Hotel\", a suggestion combining \"Glennville\" and \"Kiwanis\".'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e18857e2-4cbc-416f-aaf6-bf5834d47b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = build_llama2_prompt(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "415fa153-d9c3-42d8-bea1-b737f486954e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "[INST] <<SYS>>\n",
      "Given the following context, answer the question as accurately as possible:\n",
      "<</SYS>>\n",
      "\n",
      "### Question\n",
      "The Oberoi family is part of a hotel company that has a head office in what city?\n",
      "\n",
      "### Context\n",
      "The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta. It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two towers that comprises a hotel and the Airlangga Apartment respectively. The hotel was opened in 2005.\n",
      "The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.\n",
      "The Oberoi Group is a hotel company with its head office in Delhi. Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.\n",
      "The 289th Military Police Company was activated on 1 November 1994 and attached to Hotel Company, 3rd Infantry (The Old Guard), Fort Myer, Virginia. Hotel Company is the regiment's specialty company.\n",
      "The Glennwanis Hotel is a historic hotel in Glennville, Georgia, Tattnall County, Georgia, built on the site of the Hughes Hotel. The hotel is located at 209-215 East Barnard Street. The old Hughes Hotel was built out of Georgia pine circa 1905 and burned in 1920. The Glennwanis was built in brick in 1926. The local Kiwanis club led the effort to get the replacement hotel built, and organized a Glennville Hotel Company with directors being local business leaders. The wife of a local doctor won a naming contest with the name \"Glennwanis Hotel\", a suggestion combining \"Glennville\" and \"Kiwanis\".'[/INST] \n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a3e87-3556-4692-8a50-7296b90a2c9c",
   "metadata": {},
   "source": [
    "Run a prediction with inference configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15e564c5-e405-4c1f-83c2-2cf73953a895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "  }\n",
    "\n",
    "output = llm.predict({\"text\":input, \"properties\" : params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e346701-2986-44d4-87ce-404b0d2102ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context provided, the answer to the question is:\n",
      "\n",
      "The Oberoi family is part of a hotel company that has a head office in Mumbai, India.\n"
     ]
    }
   ],
   "source": [
    "print(output['outputs'][0][\"generated_text\"][len(input):]) # automatically removed the bos_token and eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837618c-7676-4361-87cd-0e04b2385014",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d55e04-c5ad-4f7e-b1d8-be7a8369a8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c18db-60f3-46b4-8d4b-ca80201bbb81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
