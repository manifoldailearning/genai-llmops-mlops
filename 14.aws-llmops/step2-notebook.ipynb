{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75779f7f-682a-4fc3-b78f-5206fab818b5",
   "metadata": {},
   "source": [
    "# Finetune an LLM on Amazon SageMaker\n",
    "In this notebook, we are going to focus on 3 topics:\n",
    "\n",
    "1. Process a public available dataset for LLM training/finetuning \n",
    "2. Finetune an LLM using QLoRA, an efficient finetuning technique that matches the performance of full-precision fine-tuning approaches.\n",
    "3. Deploy the finetuned LLM for inference using SageMaker.\n",
    "\n",
    "For preprocessing the dataset, we use a SageMaker Processing job to help provide the compute resources required to complete the processing steps.\n",
    "\n",
    "For model finetuning, we'll be using a SageMaker Training job to automatically spins up compute resources, execute the model training steps, and shutdown the resources automatically when the job is complete. \n",
    "\n",
    "To deploy the finetuned model, we'll be using the SageMaker Python SDK to deploy the model into SageMaker for a fully managed HTTPS endpoint in a single command.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94ee4f-d395-42d1-ae39-838e1d9854c7",
   "metadata": {},
   "source": [
    "First, we need to install the dependencies needed to run the notebook end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a433f40d-5dbb-41b6-82b4-bea95d81fe09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 datasets pygments -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c8845e-6810-4872-861e-27758fc31285",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::866824485776:role/service-role/AmazonSageMaker-ExecutionRole-20240725T121088\n",
      "sagemaker bucket: sagemaker-us-east-1-866824485776\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.local import LocalSession\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sagemaker.experiments.run import Run\n",
    "import uuid\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93f1dc4-2b14-44d2-ac48-526f9e9fa74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaa9e7-7aef-4f1f-8e8e-7cf6753a34e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff28fed-c466-4e1e-baae-788e9d59988a",
   "metadata": {},
   "source": [
    "Define the variables to be used for the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e448f00c-75f8-4293-9365-0d243853448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  \"get_ipython().run_line_magic('pip', 'install sagemaker boto3 datasets pygments -U -q')\",\n",
       "  'import sagemaker\\nimport boto3\\nfrom sagemaker.local import LocalSession\\nimport os\\nfrom datetime import datetime\\nfrom sagemaker.experiments.run import Run\\nimport uuid\\n\\nsess = sagemaker.Session()\\nregion = sess.boto_region_name\\nsm_client = boto3.client(\"sagemaker\")\\n\\n# sagemaker session bucket -> used for uploading data, models and logs\\n# sagemaker will automatically create this bucket if it not exists\\nsagemaker_session_bucket=None\\nif sagemaker_session_bucket is None and sess is not None:\\n    # set to default bucket if a bucket name is not given\\n    sagemaker_session_bucket = sess.default_bucket()\\n\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client(\\'iam\\')\\n    role = iam.get_role(RoleName=\\'sagemaker_execution_role\\')[\\'Role\\'][\\'Arn\\']\\n\\n\\nprint(f\"sagemaker role arn: {role}\")\\nprint(f\"sagemaker bucket: {sess.default_bucket()}\")\\nprint(f\"sagemaker session region: {region}\")',\n",
       "  'from sagemaker.huggingface import HuggingFaceProcessor\\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\\nfrom sagemaker import get_execution_role',\n",
       "  'locals()'],\n",
       " '_oh': {},\n",
       " '_dh': [PosixPath('/home/sagemaker-user/demo')],\n",
       " 'In': ['',\n",
       "  \"get_ipython().run_line_magic('pip', 'install sagemaker boto3 datasets pygments -U -q')\",\n",
       "  'import sagemaker\\nimport boto3\\nfrom sagemaker.local import LocalSession\\nimport os\\nfrom datetime import datetime\\nfrom sagemaker.experiments.run import Run\\nimport uuid\\n\\nsess = sagemaker.Session()\\nregion = sess.boto_region_name\\nsm_client = boto3.client(\"sagemaker\")\\n\\n# sagemaker session bucket -> used for uploading data, models and logs\\n# sagemaker will automatically create this bucket if it not exists\\nsagemaker_session_bucket=None\\nif sagemaker_session_bucket is None and sess is not None:\\n    # set to default bucket if a bucket name is not given\\n    sagemaker_session_bucket = sess.default_bucket()\\n\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client(\\'iam\\')\\n    role = iam.get_role(RoleName=\\'sagemaker_execution_role\\')[\\'Role\\'][\\'Arn\\']\\n\\n\\nprint(f\"sagemaker role arn: {role}\")\\nprint(f\"sagemaker bucket: {sess.default_bucket()}\")\\nprint(f\"sagemaker session region: {region}\")',\n",
       "  'from sagemaker.huggingface import HuggingFaceProcessor\\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\\nfrom sagemaker import get_execution_role',\n",
       "  'locals()'],\n",
       " 'Out': {},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7feb031c50c0>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7feb031c5d50>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7feb031c5d50>,\n",
       " 'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       " '_': '',\n",
       " '__': '',\n",
       " '___': '',\n",
       " '__session__': '/home/sagemaker-user/demo/step2-notebook.ipynb',\n",
       " '_i': 'from sagemaker.huggingface import HuggingFaceProcessor\\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\\nfrom sagemaker import get_execution_role',\n",
       " '_ii': 'import sagemaker\\nimport boto3\\nfrom sagemaker.local import LocalSession\\nimport os\\nfrom datetime import datetime\\nfrom sagemaker.experiments.run import Run\\nimport uuid\\n\\nsess = sagemaker.Session()\\nregion = sess.boto_region_name\\nsm_client = boto3.client(\"sagemaker\")\\n\\n# sagemaker session bucket -> used for uploading data, models and logs\\n# sagemaker will automatically create this bucket if it not exists\\nsagemaker_session_bucket=None\\nif sagemaker_session_bucket is None and sess is not None:\\n    # set to default bucket if a bucket name is not given\\n    sagemaker_session_bucket = sess.default_bucket()\\n\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client(\\'iam\\')\\n    role = iam.get_role(RoleName=\\'sagemaker_execution_role\\')[\\'Role\\'][\\'Arn\\']\\n\\n\\nprint(f\"sagemaker role arn: {role}\")\\nprint(f\"sagemaker bucket: {sess.default_bucket()}\")\\nprint(f\"sagemaker session region: {region}\")',\n",
       " '_iii': '%pip install sagemaker boto3 datasets pygments -U -q',\n",
       " '_i1': '%pip install sagemaker boto3 datasets pygments -U -q',\n",
       " '_exit_code': 0,\n",
       " '_i2': 'import sagemaker\\nimport boto3\\nfrom sagemaker.local import LocalSession\\nimport os\\nfrom datetime import datetime\\nfrom sagemaker.experiments.run import Run\\nimport uuid\\n\\nsess = sagemaker.Session()\\nregion = sess.boto_region_name\\nsm_client = boto3.client(\"sagemaker\")\\n\\n# sagemaker session bucket -> used for uploading data, models and logs\\n# sagemaker will automatically create this bucket if it not exists\\nsagemaker_session_bucket=None\\nif sagemaker_session_bucket is None and sess is not None:\\n    # set to default bucket if a bucket name is not given\\n    sagemaker_session_bucket = sess.default_bucket()\\n\\ntry:\\n    role = sagemaker.get_execution_role()\\nexcept ValueError:\\n    iam = boto3.client(\\'iam\\')\\n    role = iam.get_role(RoleName=\\'sagemaker_execution_role\\')[\\'Role\\'][\\'Arn\\']\\n\\n\\nprint(f\"sagemaker role arn: {role}\")\\nprint(f\"sagemaker bucket: {sess.default_bucket()}\")\\nprint(f\"sagemaker session region: {region}\")',\n",
       " 'sagemaker': <module 'sagemaker' from '/opt/conda/lib/python3.10/site-packages/sagemaker/__init__.py'>,\n",
       " 'boto3': <module 'boto3' from '/opt/conda/lib/python3.10/site-packages/boto3/__init__.py'>,\n",
       " 'LocalSession': sagemaker.local.local_session.LocalSession,\n",
       " 'os': <module 'os' from '/opt/conda/lib/python3.10/os.py'>,\n",
       " 'datetime': datetime.datetime,\n",
       " 'Run': sagemaker.experiments.run.Run,\n",
       " 'uuid': <module 'uuid' from '/opt/conda/lib/python3.10/uuid.py'>,\n",
       " 'sess': <sagemaker.session.Session at 0x7feb03262e60>,\n",
       " 'region': 'us-east-1',\n",
       " 'sm_client': <botocore.client.SageMaker at 0x7fead5b319f0>,\n",
       " 'sagemaker_session_bucket': 'sagemaker-us-east-1-866824485776',\n",
       " 'role': 'arn:aws:iam::866824485776:role/service-role/AmazonSageMaker-ExecutionRole-20240725T121088',\n",
       " '_i3': 'from sagemaker.huggingface import HuggingFaceProcessor\\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\\nfrom sagemaker import get_execution_role',\n",
       " 'HuggingFaceProcessor': sagemaker.huggingface.processing.HuggingFaceProcessor,\n",
       " 'ProcessingInput': sagemaker.processing.ProcessingInput,\n",
       " 'ProcessingOutput': sagemaker.processing.ProcessingOutput,\n",
       " 'get_execution_role': <function sagemaker.session.get_execution_role(sagemaker_session=None, use_default=False)>,\n",
       " '_i4': 'locals()'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8b1461-d967-42c8-9f17-19fa5f8c951d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"base_model_pkg_group_name\" not in locals():\n",
    "    base_model_pkg_group_name = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54553e0f-ea37-42ce-9a5c-4b71bf4b944b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset_s3_loc: s3://sagemaker-us-east-1-866824485776/data/bootcamp-4854f/train\n",
      "validation_dataset_s3_loc: s3://sagemaker-us-east-1-866824485776/data/bootcamp-4854f/eval\n",
      "model artifact S3 location: s3://sagemaker-us-east-1-866824485776/data/bootcamp-4854f/model\n",
      "model evaluation output S3 location: s3://sagemaker-us-east-1-866824485776/data/bootcamp-4854f/modeleval\n",
      "model_id: NousResearch/Llama-2-7b-chat-hf\n",
      "base model package group name: None\n",
      "Huggingfae dataset name: hotpot_qa\n"
     ]
    }
   ],
   "source": [
    "rand_id = uuid.uuid4().hex[:5] # this is the random-id assigned for each run. \n",
    "training_dataset_s3_loc = f\"s3://{sagemaker_session_bucket}/data/bootcamp-{rand_id}/train\"\n",
    "validation_dataset_s3_loc = f\"s3://{sagemaker_session_bucket}/data/bootcamp-{rand_id}/eval\"\n",
    "model_output_s3_loc = f\"s3://{sagemaker_session_bucket}/data/bootcamp-{rand_id}/model\"\n",
    "model_eval_s3_loc = f\"s3://{sagemaker_session_bucket}/data/bootcamp-{rand_id}/modeleval\"\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "hf_dataset_name = \"hotpot_qa\"\n",
    "\n",
    "print(f\"training_dataset_s3_loc: {training_dataset_s3_loc}\")\n",
    "print(f\"validation_dataset_s3_loc: {validation_dataset_s3_loc}\")\n",
    "print(f\"model artifact S3 location: {model_output_s3_loc}\")\n",
    "print(f\"model evaluation output S3 location: {model_eval_s3_loc}\")\n",
    "print(f\"model_id: {model_id}\")\n",
    "print(f\"base model package group name: {base_model_pkg_group_name}\")\n",
    "print(f\"Huggingfae dataset name: {hf_dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad14044-95a2-4d01-b182-1503bb449d00",
   "metadata": {},
   "source": [
    "# Proprocessing Data\n",
    "In our bootcamp, we'll build a generative AI chatbot application which requires the LLM the ability to understand instructions, and to provide accurate answer based on user query in natural language. \n",
    "For this reason, we choose an open source Llama2 base model [NousResearch-Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf) which has been instruction tuned. We will finetune this model using good quality Q&A dataset. \n",
    "\n",
    "For our hands-on, we'll use a public dataset called [hotpotQA](https://hotpotqa.github.io/) as the data source. Here's a short summary of the dataset: \n",
    "\n",
    "HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. It is collected by a team of NLP researchers at Carnegie Mellon University, Stanford University, and Université de Montréal.\n",
    "\n",
    "## SageMaker Processing\n",
    "\n",
    "To analyze data and evaluate machine learning models on Amazon SageMaker, we use a Amazon SageMaker Processing job. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. You can also use the Amazon SageMaker Processing APIs during the experimentation phase and after the code is deployed in production to evaluate performance.\n",
    "\n",
    "Here's a diagram that depicts how SageMaker Processing work:\n",
    "\n",
    "![sagemaker-processing](images/sagemaker-processing-diagram.png)\n",
    "\n",
    "In particular, we'll leverage a python script which contains the required code to handle the dataset. The script is executed in a Sagemaker processing job to automate the task end to end. The processing script can be shown in the following, and accessible in [src/preprocess/preprocess.py](src/preprocess/preprocess.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ae9c6-abe0-4fa1-bed7-a786badbf8b4",
   "metadata": {},
   "source": [
    "In the next cell, we'll process the data by running the script above as a SageMaker processing job. \n",
    "\n",
    "To launch a processing job, we use a Pytorch container by executing the `PytorchProcessor.run()` method. The `run()` method supports passing the arguments to the script.\n",
    "\n",
    "You can optionally provide input data in run() method to provide an input dataset on S3 bucket. By default, SageMaker processing job will download the data from the specified S3 location into local path inside the processing container in `/opt/ml/processing/input` directory.\n",
    "\n",
    "You could also provide an S3 location for the output data via the run() method by configuring an `ProcessingOutput` object. If not provided, SageMaker processing job defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name/`. \n",
    "\n",
    "Following code shows the python script to be used for the processing job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349acdf0-84bc-4b07-b37d-873be055c90e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_dataset\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mformat_hotpot\u001b[39;49;00m(sample):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that takes a single data sample derived from Huggingface datasets API: (https://huggingface.co/docs/datasets/index)\u001b[39;49;00m\n",
      "\u001b[33m    and formats it into llama2 prompt format. For more information about llama2 prompt format, \u001b[39;49;00m\n",
      "\u001b[33m    please refer to https://huggingface.co/blog/llama2#how-to-prompt-llama-2 \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    An example prompt is shown in the following:\u001b[39;49;00m\n",
      "\u001b[33m    <s>\u001b[39;49;00m\n",
      "\u001b[33m      [INST] <<SYS>>\u001b[39;49;00m\n",
      "\u001b[33m        {{system}}\u001b[39;49;00m\n",
      "\u001b[33m      <</SYS>>\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m      ### Question\u001b[39;49;00m\n",
      "\u001b[33m      {{question}}\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m      ### Context\u001b[39;49;00m\n",
      "\u001b[33m      {{context}}[/INST] {{answer}}</s>\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    @type  sample: Dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    @param sample: dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    @rtype:   string\u001b[39;49;00m\n",
      "\u001b[33m    @return:  llama2 prompt format\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    prefix = \u001b[33m\"\u001b[39;49;00m\u001b[33m<s>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    postfix = \u001b[33m\"\u001b[39;49;00m\u001b[33m</s>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    system_start_tag = \u001b[33m\"\u001b[39;49;00m\u001b[33m<<SYS>>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    system_end_tag = \u001b[33m\"\u001b[39;49;00m\u001b[33m<</SYS>>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    instruction_start_tag = \u001b[33m\"\u001b[39;49;00m\u001b[33m[INST]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    instruction_end_tag = \u001b[33m\"\u001b[39;49;00m\u001b[33m[/INST]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    context = \u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([ \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m sample[\u001b[33m'\u001b[39;49;00m\u001b[33mcontext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33msentences\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]])\u001b[37m\u001b[39;49;00m\n",
      "    system = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mGiven the following context, answer the question as accurately as possible:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    question_prompt = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m### Question\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msample[\u001b[33m'\u001b[39;49;00m\u001b[33mquestion\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    context_prompt = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m### Context\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcontext\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    prompt = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mprefix\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00minstruction_start_tag\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msystem_start_tag\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msystem\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msystem_end_tag\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mquestion_prompt\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mcontext_prompt\u001b[33m}\u001b[39;49;00m\u001b[33m{\u001b[39;49;00minstruction_end_tag\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msample[\u001b[33m'\u001b[39;49;00m\u001b[33manswer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mpostfix\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m prompt\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# template dataset to add prompt to each sample\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtemplate_dataset\u001b[39;49;00m(sample):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Create a field for the given sample to store formatted llama2 prompt.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    @type  sample: Dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    @param sample: Dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    @rtype:   Dataset\u001b[39;49;00m\n",
      "\u001b[33m    @return:  Dataset sample\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    sample[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mformat_hotpot(sample)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m sample\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hf-dataset-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-data-split\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m:10\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval-data-split\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m:10\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mReceived arguments \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args))    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    dataset = load_dataset(args.hf_dataset_name, \u001b[33m\"\u001b[39;49;00m\u001b[33mdistractor\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, split=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain[\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.train_data_split\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, trust_remote_code=\u001b[34mTrue\u001b[39;49;00m) \u001b[37m\u001b[39;49;00m\n",
      "    new_dataset = dataset.map(template_dataset, remove_columns=\u001b[36mlist\u001b[39;49;00m(dataset.features))\u001b[37m\u001b[39;49;00m\n",
      "    training_input_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    new_dataset.save_to_disk(training_input_path)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtraining dataset uploaded to: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtraining_input_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    eval_dataset = load_dataset(args.hf_dataset_name, \u001b[33m\"\u001b[39;49;00m\u001b[33mdistractor\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, split=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain[\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.eval_data_split\u001b[33m}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, trust_remote_code=\u001b[34mTrue\u001b[39;49;00m) \u001b[37m\u001b[39;49;00m\n",
      "    new_eval_dataset = eval_dataset.map(template_dataset, remove_columns=\u001b[36mlist\u001b[39;49;00m(eval_dataset.features))\u001b[37m\u001b[39;49;00m\n",
      "    eval_input_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/eval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    new_eval_dataset.save_to_disk(eval_input_path)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33meval dataset uploaded to: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00meval_input_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/preprocess/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80da8952-740c-4d8c-baf2-cfb63f9bc5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HuggingFaceProcessor\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "\n",
    "torch_processor = PyTorchProcessor(\n",
    "    framework_version='2.0',\n",
    "    role=get_execution_role(),\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    # instance_type='local', # uncomment for local mode\n",
    "    instance_count=1,\n",
    "    base_job_name='frameworkprocessor-PT',\n",
    "    py_version=\"py310\",\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb891a82-b01f-42b2-ab11-b0c5490192b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded src/preprocess to s3://sagemaker-us-east-1-866824485776/frameworkprocessor-PT-2024-08-07-02-06-34-123/source/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-866824485776/frameworkprocessor-PT-2024-08-07-02-06-34-123/source/runproc.sh\n",
      "INFO:sagemaker:Creating processing-job with name frameworkprocessor-PT-2024-08-07-02-06-34-123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............\u001b[34mWARNING: Skipping typing as it is not installed.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 1))\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.2)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow>=15.0.0 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mCollecting requests>=2.32.2 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.66.3 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 8.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.9.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.21.2 (from datasets->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 55.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.5/417.5 kB 58.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.9/39.9 MB 63.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 9.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 25.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xxhash, tqdm, requests, pyarrow-hotfix, pyarrow, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.1\n",
      "    Uninstalling pyarrow-14.0.1:\n",
      "      Successfully uninstalled pyarrow-14.0.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-2.20.0 huggingface-hub-0.24.5 pyarrow-17.0.0 pyarrow-hotfix-0.6 requests-2.32.3 tqdm-4.66.5 xxhash-3.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.1 -> 24.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(hf_dataset_name='hotpot_qa', train_data_split='1:50', eval_data_split='51:100')\u001b[0m\n",
      "\u001b[34m#015Downloading builder script:   0%|          | 0.00/6.42k [00:00<?, ?B/s]#015Downloading builder script: 100%|██████████| 6.42k/6.42k [00:00<00:00, 28.7MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading readme:   0%|          | 0.00/9.19k [00:00<?, ?B/s]#015Downloading readme: 100%|██████████| 9.19k/9.19k [00:00<00:00, 43.1MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]#015Downloading data:   1%|          | 4.35M/566M [00:00<00:12, 43.5MB/s]#015Downloading data:   3%|▎         | 15.2M/566M [00:00<00:06, 81.6MB/s]#015Downloading data:   5%|▍         | 26.0M/566M [00:00<00:05, 93.6MB/s]#015Downloading data:   6%|▋         | 36.8M/566M [00:00<00:05, 99.4MB/s]#015Downloading data:   8%|▊         | 47.6M/566M [00:00<00:05, 103MB/s] #015Downloading data:  10%|█         | 58.4M/566M [00:00<00:04, 104MB/s]#015Downloading data:  12%|█▏        | 69.3M/566M [00:00<00:04, 106MB/s]#015Downloading data:  14%|█▍        | 80.1M/566M [00:00<00:04, 107MB/s]#015Downloading data:  16%|█▌        | 90.9M/566M [00:00<00:04, 107MB/s]#015Downloading data:  18%|█▊        | 102M/566M [00:01<00:04, 107MB/s] #015Downloading data:  20%|█▉        | 113M/566M [00:01<00:04, 108MB/s]#015Downloading data:  22%|██▏       | 123M/566M [00:01<00:04, 108MB/s]#015Downloading data:  24%|██▎       | 134M/566M [00:01<00:03, 108MB/s]#015Downloading data:  26%|██▌       | 145M/566M [00:01<00:03, 108MB/s]#015Downloading data:  28%|██▊       | 156M/566M [00:01<00:03, 108MB/s]#015Downloading data:  29%|██▉       | 167M/566M [00:01<00:03, 108MB/s]#015Downloading data:  31%|███▏      | 178M/566M [00:01<00:03, 109MB/s]#015Downloading data:  33%|███▎      | 189M/566M [00:01<00:03, 109MB/s]#015Downloading data:  35%|███▌      | 200M/566M [00:01<00:03, 109MB/s]#015Downloading data:  37%|███▋      | 210M/566M [00:02<00:03, 109MB/s]#015Downloading data:  39%|███▉      | 221M/566M [00:02<00:03, 109MB/s]#015Downloading data:  41%|████      | 232M/566M [00:02<00:03, 109MB/s]#015Downloading data:  43%|████▎     | 243M/566M [00:02<00:02, 109MB/s]#015Downloading data:  45%|████▍     | 254M/566M [00:02<00:02, 109MB/s]#015Downloading data:  47%|████▋     | 265M/566M [00:02<00:02, 109MB/s]#015Downloading data:  49%|████▊     | 276M/566M [00:02<00:02, 109MB/s]#015Downloading data:  51%|█████     | 287M/566M [00:02<00:02, 109MB/s]#015Downloading data:  53%|█████▎    | 298M/566M [00:02<00:02, 109MB/s]#015Downloading data:  54%|█████▍    | 308M/566M [00:02<00:02, 109MB/s]#015Downloading data:  56%|█████▋    | 319M/566M [00:03<00:02, 109MB/s]#015Downloading data:  58%|█████▊    | 330M/566M [00:03<00:02, 109MB/s]#015Downloading data:  60%|██████    | 341M/566M [00:03<00:02, 109MB/s]#015Downloading data:  62%|██████▏   | 352M/566M [00:03<00:01, 109MB/s]#015Downloading data:  64%|██████▍   | 363M/566M [00:03<00:01, 109MB/s]#015Downloading data:  66%|██████▌   | 374M/566M [00:03<00:01, 109MB/s]#015Downloading data:  68%|██████▊   | 385M/566M [00:03<00:01, 109MB/s]#015Downloading data:  70%|██████▉   | 396M/566M [00:03<00:01, 109MB/s]#015Downloading data:  72%|███████▏  | 407M/566M [00:03<00:01, 109MB/s]#015Downloading data:  74%|███████▎  | 418M/566M [00:03<00:01, 109MB/s]#015Downloading data:  76%|███████▌  | 429M/566M [00:04<00:01, 106MB/s]#015Downloading data:  78%|███████▊  | 439M/566M [00:04<00:01, 100MB/s]#015Downloading data:  79%|███████▉  | 450M/566M [00:04<00:01, 103MB/s]#015Downloading data:  81%|████████▏ | 461M/566M [00:04<00:01, 104MB/s]#015Downloading data:  83%|████████▎ | 472M/566M [00:04<00:00, 105MB/s]#015Downloading data:  85%|████████▌ | 482M/566M [00:04<00:00, 106MB/s]#015Downloading data:  87%|████████▋ | 493M/566M [00:04<00:00, 107MB/s]#015Downloading data:  89%|████████▉ | 504M/566M [00:04<00:00, 108MB/s]#015Downloading data:  91%|█████████ | 515M/566M [00:04<00:00, 108MB/s]#015Downloading data:  93%|█████████▎| 526M/566M [00:04<00:00, 108MB/s]#015Downloading data:  95%|█████████▍| 537M/566M [00:05<00:00, 107MB/s]#015Downloading data:  97%|█████████▋| 548M/566M [00:05<00:00, 106MB/s]#015Downloading data:  99%|█████████▉| 560M/566M [00:05<00:00, 110MB/s]#015Downloading data: 100%|██████████| 566M/566M [00:05<00:00, 107MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data:   0%|          | 0.00/46.3M [00:00<?, ?B/s]#015Downloading data:   9%|▊         | 4.00M/46.3M [00:00<00:01, 40.0MB/s]#015Downloading data:  33%|███▎      | 15.3M/46.3M [00:00<00:00, 82.9MB/s]#015Downloading data:  57%|█████▋    | 26.2M/46.3M [00:00<00:00, 95.0MB/s]#015Downloading data:  80%|████████  | 37.1M/46.3M [00:00<00:00, 101MB/s] #015Downloading data: 100%|██████████| 46.3M/46.3M [00:00<00:00, 95.7MB/s]\u001b[0m\n",
      "\u001b[34m#015Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]#015Generating train split:   0%|          | 1/90447 [00:05<139:06:45,  5.54s/ examples]#015Generating train split:   1%|          | 528/90447 [00:05<11:17, 132.72 examples/s] #015Generating train split:   1%|          | 1000/90447 [00:05<05:09, 289.25 examples/s]#015Generating train split:   2%|▏         | 1600/90447 [00:05<02:39, 556.57 examples/s]#015Generating train split:   3%|▎         | 2299/90447 [00:06<01:35, 927.51 examples/s]#015Generating train split:   3%|▎         | 2893/90447 [00:06<01:06, 1325.25 examples/s]#015Generating train split:   4%|▍         | 3599/90447 [00:06<00:48, 1805.44 examples/s]#015Generating train split:   5%|▍         | 4302/90447 [00:06<00:37, 2281.87 examples/s]#015Generating train split:   5%|▌         | 4896/90447 [00:06<00:30, 2791.09 examples/s]#015Generating train split:   6%|▌         | 5604/90447 [00:06<00:26, 3231.10 examples/s]#015Generating train split:   7%|▋         | 6301/90447 [00:06<00:23, 3556.05 examples/s]#015Generating train split:   8%|▊         | 6891/90447 [00:06<00:20, 3993.90 examples/s]#015Generating train split:   8%|▊         | 7597/90447 [00:07<00:19, 4146.07 examples/s]#015Generating train split:   9%|▉         | 8298/90447 [00:07<00:19, 4230.55 examples/s]#015Generating train split:  10%|▉         | 8891/90447 [00:07<00:17, 4590.48 examples/s]#015Generating train split:  11%|█         | 9590/90447 [00:07<00:18, 4475.76 examples/s]#015Generating train split:  11%|█▏        | 10302/90447 [00:07<00:17, 4484.80 examples/s]#015Generating train split:  12%|█▏        | 10892/90447 [00:07<00:16, 4793.52 examples/s]#015Generating train split:  13%|█▎        | 11598/90447 [00:07<00:16, 4688.22 examples/s]#015Generating train split:  14%|█▎        | 12302/90447 [00:08<00:16, 4625.35 examples/s]#015Generating train split:  14%|█▍        | 12896/90447 [00:08<00:15, 4912.35 examples/s]#015Generating train split:  15%|█▌        | 13593/90447 [00:08<00:16, 4746.97 examples/s]#015Generating train split:  16%|█▌        | 14302/90447 [00:08<00:16, 4673.19 examples/s]#015Generating train split:  16%|█▋        | 14893/90447 [00:08<00:15, 4945.54 examples/s]#015Generating train split:  17%|█▋        | 15602/90447 [00:08<00:15, 4805.08 examples/s]#015Generating train split:  18%|█▊        | 16300/90447 [00:08<00:15, 4688.42 examples/s]#015Generating train split:  19%|█▊        | 16894/90447 [00:09<00:14, 4960.92 examples/s]#015Generating train split:  19%|█▉        | 17602/90447 [00:09<00:15, 4812.25 examples/s]#015Generating train split:  20%|██        | 18301/90447 [00:09<00:15, 4704.13 examples/s]#015Generating train split:  21%|██        | 18893/90447 [00:09<00:14, 4972.11 examples/s]#015Generating train split:  22%|██▏       | 19597/90447 [00:09<00:14, 4824.37 examples/s]#015Generating train split:  22%|██▏       | 20296/90447 [00:09<00:14, 4701.45 examples/s]#015Generating train split:  23%|██▎       | 20891/90447 [00:09<00:13, 4985.81 examples/s]#015Generating train split:  24%|██▍       | 21598/90447 [00:10<00:14, 4787.08 examples/s]#015Generating train split:  25%|██▍       | 22305/90447 [00:10<00:26, 2540.62 examples/s]#015Generating train split:  25%|██▌       | 22897/90447 [00:10<00:22, 3004.15 examples/s]#015Generating train split:  26%|██▌       | 23599/90447 [00:10<00:19, 3351.17 examples/s]#015Generating train split:  27%|██▋       | 24287/90447 [00:10<00:18, 3593.66 examples/s]#015Generating train split:  28%|██▊       | 24876/90447 [00:11<00:16, 4020.33 examples/s]#015Generating train split:  28%|██▊       | 25600/90447 [00:11<00:15, 4205.61 examples/s]#015Generating train split:  29%|██▉       | 26301/90447 [00:11<00:14, 4303.56 examples/s]#015Generating train split:  30%|██▉       | 26896/90447 [00:11<00:13, 4648.11 examples/s]#015Generating train split:  31%|███       | 27597/90447 [00:11<00:13, 4562.10 examples/s]#015Generating train split:  31%|███▏      | 28296/90447 [00:11<00:13, 4493.27 examples/s]#015Generating train split:  32%|███▏      | 28894/90447 [00:11<00:12, 4808.20 examples/s]#015Generating train split:  33%|███▎      | 29595/90447 [00:12<00:13, 4674.66 examples/s]#015Generating train split:  33%|███▎      | 30299/90447 [00:12<00:13, 4584.21 examples/s]#015Generating train split:  34%|███▍      | 30881/90447 [00:12<00:12, 4858.45 examples/s]#015Generating train split:  35%|███▍      | 31574/90447 [00:12<00:12, 4654.78 examples/s]#015Generating train split:  36%|███▌      | 32295/90447 [00:12<00:12, 4629.63 examples/s]#015Generating train split:  36%|███▋      | 32887/90447 [00:12<00:11, 4919.54 examples/s]#015Generating train split:  37%|███▋      | 33600/90447 [00:12<00:11, 4759.71 examples/s]#015Generating train split:  38%|███▊      | 34303/90447 [00:13<00:12, 4648.53 examples/s]#015Generating train split:  39%|███▊      | 34891/90447 [00:13<00:11, 4921.53 examples/s]#015Generating train split:  39%|███▉      | 35598/90447 [00:13<00:11, 4784.76 examples/s]#015Generating train split:  40%|████      | 36302/90447 [00:13<00:11, 4688.20 examples/s]#015Generating train split:  41%|████      | 36896/90447 [00:13<00:10, 4960.45 examples/s]#015Generating train split:  42%|████▏     | 37597/90447 [00:13<00:11, 4791.78 examples/s]#015Generating train split:  42%|████▏     | 38306/90447 [00:13<00:11, 4650.05 examples/s]#015Generating train split:  43%|████▎     | 38897/90447 [00:14<00:10, 4934.87 examples/s]#015Generating train split:  44%|████▍     | 39589/90447 [00:14<00:10, 4774.43 examples/s]#015Generating train split:  45%|████▍     | 40302/90447 [00:14<00:10, 4656.59 examples/s]#015Generating train split:  45%|████▌     | 40895/90447 [00:14<00:10, 4931.48 examples/s]#015Generating train split:  46%|████▌     | 41596/90447 [00:14<00:10, 4737.99 examples/s]#015Generating train split:  47%|████▋     | 42300/90447 [00:14<00:10, 4595.24 examples/s]#015Generating train split:  47%|████▋     | 42896/90447 [00:14<00:09, 4887.37 examples/s]#015Generating train split:  48%|████▊     | 43595/90447 [00:15<00:09, 4719.02 examples/s]#015Generating train split:  49%|████▉     | 44301/90447 [00:15<00:09, 4689.60 examples/s]#015Generating train split:  50%|████▉     | 44895/90447 [00:15<00:09, 4957.27 examples/s]#015Generating train split:  50%|█████     | 45601/90447 [00:15<00:09, 4790.71 examples/s]#015Generating train split:  51%|█████     | 46297/90447 [00:15<00:09, 4653.63 examples/s]#015Generating train split:  52%|█████▏    | 46890/90447 [00:15<00:08, 4942.30 examples/s]#015Generating train split:  53%|█████▎    | 47596/90447 [00:15<00:08, 4763.63 examples/s]#015Generating train split:  53%|█████▎    | 48291/90447 [00:16<00:09, 4636.14 examples/s]#015Generating train split:  54%|█████▍    | 48887/90447 [00:16<00:08, 4935.81 examples/s]#015Generating train split:  55%|█████▍    | 49596/90447 [00:16<00:08, 4735.49 examples/s]#015Generating train split:  56%|█████▌    | 50300/90447 [00:16<00:08, 4637.14 examples/s]#015Generating train split:  56%|█████▋    | 50893/90447 [00:16<00:08, 4919.74 examples/s]#015Generating train split:  57%|█████▋    | 51600/90447 [00:16<00:08, 4817.00 examples/s]#015Generating train split:  58%|█████▊    | 52302/90447 [00:16<00:08, 4761.95 examples/s]#015Generating train split:  58%|█████▊    | 52891/90447 [00:16<00:07, 5009.78 examples/s]#015Generating train split:  59%|█████▉    | 53595/90447 [00:17<00:07, 4811.93 examples/s]#015Generating train split:  60%|██████    | 54302/90447 [00:17<00:07, 4707.98 examples/s]#015Generating train split:  61%|██████    | 54894/90447 [00:17<00:07, 4976.13 examples/s]#015Generating train split:  61%|██████▏   | 55597/90447 [00:17<00:07, 4766.41 examples/s]#015Generating train split:  62%|██████▏   | 56306/90447 [00:17<00:07, 4648.71 examples/s]#015Generating train split:  63%|██████▎   | 56896/90447 [00:17<00:06, 4930.23 examples/s]#015Generating train split:  64%|██████▎   | 57599/90447 [00:17<00:06, 4780.61 examples/s]#015Generating train split:  64%|██████▍   | 58298/90447 [00:18<00:06, 4663.96 examples/s]#015Generating train split:  65%|██████▌   | 58896/90447 [00:18<00:06, 4961.84 examples/s]#015Generating train split:  66%|██████▌   | 59596/90447 [00:18<00:06, 4797.16 examples/s]#015Generating train split:  67%|██████▋   | 60302/90447 [00:18<00:11, 2546.45 examples/s]#015Generating train split:  67%|██████▋   | 60891/90447 [00:19<00:09, 3008.92 examples/s]#015Generating train split:  68%|██████▊   | 61595/90447 [00:19<00:08, 3357.96 examples/s]#015Generating train split:  69%|██████▉   | 62308/90447 [00:19<00:07, 3647.80 examples/s]#015Generating train split:  70%|██████▉   | 62895/90447 [00:19<00:06, 4061.41 examples/s]#015Generating train split:  70%|███████   | 63592/90447 [00:19<00:06, 4156.82 examples/s]#015Generating train split:  71%|███████   | 64298/90447 [00:19<00:06, 4265.47 examples/s]#015Generating train split:  72%|███████▏  | 64891/90447 [00:19<00:05, 4604.24 examples/s]#015Generating train split:  73%|███████▎  | 65596/90447 [00:20<00:05, 4564.32 examples/s]#015Generating train split:  73%|███████▎  | 66300/90447 [00:20<00:05, 4587.89 examples/s]#015Generating train split:  74%|███████▍  | 66891/90447 [00:20<00:04, 4881.92 examples/s]#015Generating train split:  75%|███████▍  | 67589/90447 [00:20<00:04, 4695.23 examples/s]#015Generating train split:  76%|███████▌  | 68300/90447 [00:20<00:04, 4608.63 examples/s]#015Generating train split:  76%|███████▌  | 68889/90447 [00:20<00:04, 4895.69 examples/s]#015Generating train split:  77%|███████▋  | 69592/90447 [00:20<00:04, 4766.99 examples/s]#015Generating train split:  78%|███████▊  | 70288/90447 [00:20<00:04, 4626.25 examples/s]#015Generating train split:  78%|███████▊  | 70880/90447 [00:21<00:03, 4917.63 examples/s]#015Generating train split:  79%|███████▉  | 71592/90447 [00:21<00:03, 4788.70 examples/s]#015Generating train split:  80%|███████▉  | 72301/90447 [00:21<00:03, 4656.21 examples/s]#015Generating train split:  81%|████████  | 72890/90447 [00:21<00:03, 4922.77 examples/s]#015Generating train split:  81%|████████▏ | 73595/90447 [00:21<00:03, 4801.83 examples/s]#015Generating train split:  82%|████████▏ | 74296/90447 [00:21<00:03, 4683.06 examples/s]#015Generating train split:  83%|████████▎ | 74887/90447 [00:21<00:03, 4961.80 examples/s]#015Generating train split:  84%|████████▎ | 75597/90447 [00:22<00:03, 4778.12 examples/s]#015Generating train split:  84%|████████▍ | 76307/90447 [00:22<00:02, 4754.19 examples/s]#015Generating train split:  85%|████████▌ | 76895/90447 [00:22<00:02, 5010.03 examples/s]#015Generating train split:  86%|████████▌ | 77601/90447 [00:22<00:02, 4835.85 examples/s]#015Generating train split:  87%|████████▋ | 78299/90447 [00:22<00:02, 4666.22 examples/s]#015Generating train split:  87%|████████▋ | 78891/90447 [00:22<00:02, 4950.65 examples/s]#015Generating train split:  88%|████████▊ | 79598/90447 [00:22<00:02, 4761.22 examples/s]#015Generating train split:  89%|████████▉ | 80295/90447 [00:23<00:02, 4650.49 examples/s]#015Generating train split:  89%|████████▉ | 80886/90447 [00:23<00:01, 4933.76 examples/s]#015Generating train split:  90%|█████████ | 81599/90447 [00:23<00:01, 4720.12 examples/s]#015Generating train split:  91%|█████████ | 82297/90447 [00:23<00:01, 4640.80 examples/s]#015Generating train split:  92%|█████████▏| 82889/90447 [00:23<00:01, 4930.31 examples/s]#015Generating train split:  92%|█████████▏| 83591/90447 [00:23<00:01, 4770.29 examples/s]#015Generating train split:  93%|█████████▎| 84301/90447 [00:23<00:01, 4717.54 examples/s]#015Generating train split:  94%|█████████▍| 84891/90447 [00:24<00:01, 4973.56 examples/s]#015Generating train split:  95%|█████████▍| 85599/90447 [00:24<00:01, 4796.56 examples/s]#015Generating train split:  95%|█████████▌| 86303/90447 [00:24<00:00, 4710.72 examples/s]#015Generating train split:  96%|█████████▌| 86894/90447 [00:24<00:00, 4977.30 examples/s]#015Generating train split:  97%|█████████▋| 87591/90447 [00:24<00:00, 4774.94 examples/s]#015Generating train split:  98%|█████████▊| 88301/90447 [00:24<00:00, 4681.76 examples/s]#015Generating train split:  98%|█████████▊| 88890/90447 [00:24<00:00, 4955.40 examples/s]#015Generating train split:  99%|█████████▉| 89601/90447 [00:25<00:00, 4793.76 examples/s]#015Generating train split: 100%|█████████▉| 90302/90447 [00:25<00:00, 4635.75 examples/s]#015Generating train split: 100%|██████████| 90447/90447 [00:25<00:00, 3527.66 examples/s]\u001b[0m\n",
      "\u001b[34m#015Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]#015Generating validation split:   0%|          | 1/7405 [00:00<42:03,  2.93 examples/s]#015Generating validation split:   8%|▊         | 588/7405 [00:00<00:03, 1735.43 examples/s]#015Generating validation split:  14%|█▎        | 1000/7405 [00:00<00:02, 2380.99 examples/s]#015Generating validation split:  21%|██▏       | 1588/7405 [00:00<00:02, 2814.29 examples/s]#015Generating validation split:  27%|██▋       | 2000/7405 [00:00<00:01, 3095.47 examples/s]#015Generating validation split:  35%|███▌      | 2601/7405 [00:00<00:01, 3865.70 examples/s]#015Generating validation split:  45%|████▍     | 3298/7405 [00:01<00:01, 4077.57 examples/s]#015Generating validation split:  53%|█████▎    | 3890/7405 [00:01<00:00, 4541.96 examples/s]#015Generating validation split:  62%|██████▏   | 4591/7405 [00:01<00:00, 4486.37 examples/s]#015Generating validation split:  72%|███████▏  | 5297/7405 [00:01<00:00, 4436.07 examples/s]#015Generating validation split:  80%|███████▉  | 5896/7405 [00:01<00:00, 4796.58 examples/s]#015Generating validation split:  89%|████████▉ | 6582/7405 [00:01<00:00, 4605.40 examples/s]#015Generating validation split:  98%|█████████▊| 7290/7405 [00:01<00:00, 4518.86 examples/s]#015Generating validation split: 100%|██████████| 7405/7405 [00:02<00:00, 3697.67 examples/s]\u001b[0m\n",
      "\u001b[34m#015Map:   0%|          | 0/49 [00:00<?, ? examples/s]#015Map: 100%|██████████| 49/49 [00:00<00:00, 3876.44 examples/s]\u001b[0m\n",
      "\u001b[34m#015Saving the dataset (0/1 shards):   0%|          | 0/49 [00:00<?, ? examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 20661.60 examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 19543.64 examples/s]\u001b[0m\n",
      "\u001b[34mtraining dataset uploaded to: /opt/ml/processing/train\u001b[0m\n",
      "\u001b[34m#015Map:   0%|          | 0/49 [00:00<?, ? examples/s]#015Map: 100%|██████████| 49/49 [00:00<00:00, 3911.26 examples/s]\u001b[0m\n",
      "\u001b[34m#015Saving the dataset (0/1 shards):   0%|          | 0/49 [00:00<?, ? examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 21209.59 examples/s]#015Saving the dataset (1/1 shards): 100%|██████████| 49/49 [00:00<00:00, 20029.32 examples/s]\u001b[0m\n",
      "\u001b[34meval dataset uploaded to: /opt/ml/processing/eval\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch_processor.run(\n",
    "    code=\"preprocess.py\",\n",
    "    source_dir=\"src/preprocess\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\",\n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=training_dataset_s3_loc),\n",
    "        ProcessingOutput(output_name=\"eval_data\",\n",
    "                         source=\"/opt/ml/processing/eval\",\n",
    "                         destination=validation_dataset_s3_loc),\n",
    "\n",
    "    ],\n",
    "    arguments=[\"--train-data-split\", \"1:50\",\n",
    "               \"--eval-data-split\", \"51:100\",\n",
    "               \"--hf-dataset-name\", hf_dataset_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f0c16-0580-4b54-b918-90ec46326a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-Tune Llama2-7b model on Amazon SageMaker\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. \n",
    "QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. \n",
    "The TL;DR; of how QLoRA works is:\n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a train.py, which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code.\n",
    "\n",
    "Here's an animation that shows how how QLoRA works in general.\n",
    "\n",
    "![lora-animated](images/lora-animated.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ed79eaf-b53d-4c78-b3a4-4fe04acf02e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\u001b[37m\u001b[39;49;00m\n",
      "    AutoModelForCausalLM,\u001b[37m\u001b[39;49;00m\n",
      "    AutoTokenizer,\u001b[37m\u001b[39;49;00m\n",
      "    BitsAndBytesConfig,\u001b[37m\u001b[39;49;00m\n",
      "    HfArgumentParser,\u001b[37m\u001b[39;49;00m\n",
      "    TrainingArguments,\u001b[37m\u001b[39;49;00m\n",
      "    pipeline,\u001b[37m\u001b[39;49;00m\n",
      "    logging,\u001b[37m\u001b[39;49;00m\n",
      ")\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpeft\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LoraConfig, PeftModel\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtrl\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SFTTrainer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mhuggingface\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m HuggingFaceModel\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcollection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Collection\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtqdm\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m tqdm\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msmexperiments_callback\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerExperimentsCallback\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mbotocore\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mexceptions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClientError\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36murllib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m urlparse\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mbitsandbytes\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mbnb\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Output directory where the model predictions and checkpoints will be stored\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "output_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/model/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "base_model_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/basemodel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "model_eval_save_dir = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/eval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# Load the entire model on the GPU 0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "device_map = \u001b[33m\"\u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_arge\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"Parse the arguments.\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mgoogle/flan-t5-xl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mModel id to use for training.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of epochs to train for.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,  help=\u001b[33m\"\u001b[39;49;00m\u001b[33mUse fp16 for training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mper device training batch size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mper device evaluation batch size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--gradient_accumulation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of batches to accumulate for gradients before optimization step\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--gradient_checkpointing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mapply gradient checkpointing for moemory optimization at training time\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_grad_norm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.3\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgradient norm clipping value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m2e-4\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--weight_decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mweight decay\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mpaged_adamw_32bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33moptimizer to use\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lr_scheduler_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mconstant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate scheduler type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=-\u001b[34m1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmaximum steps to train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.03\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate warm up ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--group_by_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgroupping datase by length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--save_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m25\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of training steps before saving a checkpoint\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--logging_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m25\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of steps before logging the training metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmaximum sequence length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--packing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mpack multiple examples into input sequence\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_r\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mLoRA attention dimension\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_alpha\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mAlpha parameter for LoRA scaling\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_dropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mDropout probability for LoRA layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_4bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mWhether to use 4bit quantization\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--bnb_4bit_compute_dtype\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mcompute dtype for bnb_4bit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--bnb_4bit_quant_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mnf4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mQuantization type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_nested_quant\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mactivate nested quantization for 4bit base models (double quantization\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--lora_bias\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to use bias in the lora adapter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--merge_weights\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mWhether to merge LoRA weights with base model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--base_model_group_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mOptional base model group name.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--region\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mus-east-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mthe region where the training job is run.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm_train_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/training\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm_validation_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/input/data/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_eval_s3_loc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_experiment\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    args = parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m args\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32ms3_download\u001b[39;49;00m(s3_bucket, s3_object_key, local_file_name, s3_client=boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that downloads an object from S3 into local filesystem using boto3 library.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    meta_data = s3_client.head_object(Bucket=s3_bucket, Key=s3_object_key)\u001b[37m\u001b[39;49;00m\n",
      "    total_length = \u001b[36mint\u001b[39;49;00m(meta_data.get(\u001b[33m'\u001b[39;49;00m\u001b[33mContentLength\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m tqdm(total=total_length,  \u001b[37m\u001b[39;49;00m\n",
      "              desc=\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33msource: s3://\u001b[39;49;00m\u001b[33m{\u001b[39;49;00ms3_bucket\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00ms3_object_key\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              bar_format=\u001b[33m\"\u001b[39;49;00m\u001b[33m{percentage:.1f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m|\u001b[39;49;00m\u001b[33m{bar:25}\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{rate_fmt}\u001b[39;49;00m\u001b[33m | \u001b[39;49;00m\u001b[33m{desc}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,  \u001b[37m\u001b[39;49;00m\n",
      "              unit=\u001b[33m'\u001b[39;49;00m\u001b[33mB\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              unit_scale=\u001b[34mTrue\u001b[39;49;00m, \u001b[37m\u001b[39;49;00m\n",
      "              unit_divisor=\u001b[34m1024\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "             ) \u001b[34mas\u001b[39;49;00m pbar:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(local_file_name, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "            s3_client.download_fileobj(s3_bucket, s3_object_key, f, Callback=pbar.update)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mdownload_and_untar_s3_tar\u001b[39;49;00m(destination_path, source_s3_path):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that downloads a file on S3, then untar the file in local filesystem.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    src_s3_bucket = source_s3_path.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m2\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    src_s3_prefix = \u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(source_s3_path.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[\u001b[34m3\u001b[39;49;00m:])\u001b[37m\u001b[39;49;00m\n",
      "    destination_file_path = os.path.join(destination_path, os.path.basename(source_s3_path))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mDownloading file from \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msrc_s3_bucket\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00msrc_s3_prefix\u001b[33m}\u001b[39;49;00m\u001b[33m to \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdestination_file_path\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    s3_download(\u001b[37m\u001b[39;49;00m\n",
      "        s3_bucket=src_s3_bucket,\u001b[37m\u001b[39;49;00m\n",
      "        s3_object_key=src_s3_prefix,\u001b[37m\u001b[39;49;00m\n",
      "        local_file_name=destination_file_path\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Create a tarfile object and extract the contents to the local disk\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tar = tarfile.open(destination_file_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    tar.extractall(path=destination_path)\u001b[37m\u001b[39;49;00m\n",
      "    tar.close()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_data_uri_from_model_package\u001b[39;49;00m(model_group_name, region=\u001b[33m\"\u001b[39;49;00m\u001b[33mus-east-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Function that retrieves the model artifact for the given model group name.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    sagemaker_session = sagemaker.session.Session(boto3.session.Session(region_name=region))\u001b[37m\u001b[39;49;00m\n",
      "    region = sagemaker_session.boto_region_name\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    sm_client = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, region_name=region)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_packages = sm_client.list_model_packages(\u001b[37m\u001b[39;49;00m\n",
      "        ModelPackageGroupName=model_group_name\u001b[37m\u001b[39;49;00m\n",
      "    )[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageSummaryList\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_package_name = \u001b[36msorted\u001b[39;49;00m(\u001b[37m\u001b[39;49;00m\n",
      "        [\u001b[37m\u001b[39;49;00m\n",
      "            (package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageVersion\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelPackageArn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]) \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m package \u001b[35min\u001b[39;49;00m model_packages \u001b[34mif\u001b[39;49;00m package[\u001b[33m'\u001b[39;49;00m\u001b[33mModelApprovalStatus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[33m'\u001b[39;49;00m\u001b[33mApproved\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\u001b[37m\u001b[39;49;00m\n",
      "        reverse=\u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )[-\u001b[34m1\u001b[39;49;00m][-\u001b[34m1\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mfound model package: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_package_name\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m sm_client.describe_model_package(\u001b[37m\u001b[39;49;00m\n",
      "        ModelPackageName=model_package_name\u001b[37m\u001b[39;49;00m\n",
      "    )[\u001b[33m'\u001b[39;49;00m\u001b[33mInferenceSpecification\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mContainers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mModelDataUrl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mquantization_config\u001b[39;49;00m(args, compute_dtype):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    At a high level, QLoRA uses 4-bit quantization to compress a pretrained language model.\u001b[39;49;00m\n",
      "\u001b[33m    This function sets up the quantization configuration for model training with QLoRA. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    The LoRA layers are the only parameters being updated during training. \u001b[39;49;00m\n",
      "\u001b[33m    Read more about LoRA in the original LoRA paper (https://arxiv.org/abs/2106.09685). \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# 4 bit configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    bnb_config = BitsAndBytesConfig(\u001b[37m\u001b[39;49;00m\n",
      "        load_in_4bit=args.use_4bit,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_compute_dtype=compute_dtype,\u001b[37m\u001b[39;49;00m\n",
      "        bnb_4bit_use_double_quant=args.use_nested_quant,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m bnb_config\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_pretrained_model\u001b[39;49;00m(args, model_name, compute_dtype):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads the pretrained model into GPU memory for finetuning. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    There are 2 modes supported: \u001b[39;49;00m\n",
      "\u001b[33m    1. Directly download a pretrained model weights\u001b[39;49;00m\n",
      "\u001b[33m    from Huggingface Hub over the public internet.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    2. Download the pretrained model weight from a base model package\u001b[39;49;00m\n",
      "\u001b[33m    registered in SageMaker Model Registry. Downloading weights from S3 \u001b[39;49;00m\n",
      "\u001b[33m    could improve the download speed significantly. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.base_model_group_name != \u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        os.makedirs(base_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        model_data_uri = model_data_uri_from_model_package(\u001b[37m\u001b[39;49;00m\n",
      "            model_group_name=args.base_model_group_name,\u001b[37m\u001b[39;49;00m\n",
      "            region=args.region\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        download_and_untar_s3_tar(\u001b[37m\u001b[39;49;00m\n",
      "        destination_path=base_model_path, \u001b[37m\u001b[39;49;00m\n",
      "        source_s3_path=model_data_uri\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    bnb_config = quantization_config(args, compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load base model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model = AutoModelForCausalLM.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "        model_name,\u001b[37m\u001b[39;49;00m\n",
      "        quantization_config=bnb_config,\u001b[37m\u001b[39;49;00m\n",
      "        device_map=device_map\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    model.config.use_cache = \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model.config.pretraining_tp = \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_tokenizer\u001b[39;49;00m(args, model_name):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads the tokenizer for llama2 model. Tokenizer is used in the training process to \u001b[39;49;00m\n",
      "\u001b[33m    convert the input texts into tokens. \u001b[39;49;00m\n",
      "\u001b[33m    Please refer to: https://huggingface.co/docs/transformers/v4.31.0/model_doc/llama2#transformers.LlamaTokenizer \u001b[39;49;00m\n",
      "\u001b[33m    to learn more about this specific tokenizer.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.pad_token = tokenizer.eos_token\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer.padding_side = \u001b[33m\"\u001b[39;49;00m\u001b[33mright\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m# Fix weird overflow issue with fp16 training\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m tokenizer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_lora_config\u001b[39;49;00m(args, modules):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Loads QLoRA configuration for the training job. \u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load LoRA configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    peft_config = LoraConfig(\u001b[37m\u001b[39;49;00m\n",
      "        lora_alpha=args.lora_alpha,\u001b[37m\u001b[39;49;00m\n",
      "        lora_dropout=args.lora_dropout,\u001b[37m\u001b[39;49;00m\n",
      "        r=args.lora_r,\u001b[37m\u001b[39;49;00m\n",
      "        bias=args.lora_bias,\u001b[37m\u001b[39;49;00m\n",
      "        task_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mCAUSAL_LM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        target_modules = modules\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m peft_config\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32ms3_upload\u001b[39;49;00m(model_evaluation_s3_path, local_file_name, s3_client=boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Uploads the given file in local file system to a specified S3 location.\u001b[39;49;00m\n",
      "\u001b[33m    This function is used for uploading the model evaluation metrics to S3.\u001b[39;49;00m\n",
      "\u001b[33m    The metrics will be used when registering the trained model with SageMaker Model Registry.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    o = urlparse(model_evaluation_s3_path)\u001b[37m\u001b[39;49;00m\n",
      "    s3_bucket = o.netloc\u001b[37m\u001b[39;49;00m\n",
      "    s3_object_key = o.path\u001b[37m\u001b[39;49;00m\n",
      "    local_base_file_name = os.path.basename(local_file_name)\u001b[37m\u001b[39;49;00m\n",
      "    s3_object_key = os.path.join(s3_object_key, local_base_file_name)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        response = s3_client.upload_file(local_file_name, s3_bucket, s3_object_key.lstrip(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m ClientError \u001b[34mas\u001b[39;49;00m e:\u001b[37m\u001b[39;49;00m\n",
      "        logging.error(e)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_evaluation\u001b[39;49;00m(args, metrics):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Captures the training and evaluation metrics from the model training exercise. \u001b[39;49;00m\n",
      "\u001b[33m    The metrics will be written to file, and copied to the specifiued S3 locaiton.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    train_loss = \u001b[36mfloat\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    eval_loss = \u001b[36mfloat\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m metric \u001b[35min\u001b[39;49;00m metrics:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m metric:\u001b[37m\u001b[39;49;00m\n",
      "            train_loss = metric[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35min\u001b[39;49;00m metric:\u001b[37m\u001b[39;49;00m\n",
      "            eval_loss = metric[\u001b[33m'\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_metrics =  {\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mregression_metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : {\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : train_loss\u001b[37m\u001b[39;49;00m\n",
      "            },\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33meval_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : {\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m : eval_loss\u001b[37m\u001b[39;49;00m\n",
      "            }\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        },\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mevaluation_metrics\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m#Save Evaluation Report\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    pathlib.Path(model_eval_save_dir).mkdir(parents=\u001b[34mTrue\u001b[39;49;00m, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_path = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_eval_save_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "        f.write(json.dumps(evaluation_metrics))\u001b[37m\u001b[39;49;00m\n",
      "    s3_upload(args.model_eval_s3_loc, evaluation_path)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# COPIED FROM https://github.com/artidoro/qlora/blob/main/qlora.py\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfind_all_linear_names\u001b[39;49;00m(model):\u001b[37m\u001b[39;49;00m\n",
      "    lora_module_names = \u001b[36mset\u001b[39;49;00m()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m name, module \u001b[35min\u001b[39;49;00m model.named_modules():\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(module, bnb.nn.Linear4bit):\u001b[37m\u001b[39;49;00m\n",
      "            names = name.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            lora_module_names.add(names[\u001b[34m0\u001b[39;49;00m] \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(names) == \u001b[34m1\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m names[-\u001b[34m1\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mlm_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m lora_module_names:  \u001b[37m# needed for 16-bit\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        lora_module_names.remove(\u001b[33m\"\u001b[39;49;00m\u001b[33mlm_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mlist\u001b[39;49;00m(lora_module_names)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtraining_function\u001b[39;49;00m(args):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mmerging weights: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.merge_weights\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.sm_train_dir)\u001b[37m\u001b[39;49;00m\n",
      "    eval_dataset = load_from_disk(args.sm_validation_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Load tokenizer and model with QLoRA configuration\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    compute_dtype = \u001b[36mgetattr\u001b[39;49;00m(torch, args.bnb_4bit_compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "    model_name = args.model_id\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.base_model_group_name != \u001b[33m\"\u001b[39;49;00m\u001b[33mNone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        model_name = base_model_path\u001b[37m\u001b[39;49;00m\n",
      "    model = load_pretrained_model(args, model_name, compute_dtype)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = load_tokenizer(args, model_name)\u001b[37m\u001b[39;49;00m\n",
      "    lora_modules = find_all_linear_names(model)\u001b[37m\u001b[39;49;00m\n",
      "    lora_config = load_lora_config(args, lora_modules)\u001b[37m\u001b[39;49;00m\n",
      "    packing = \u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.packing \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    fp16 = \u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.fp16 \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    bf16 = \u001b[34mFalse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m compute_dtype == torch.float16 \u001b[35mand\u001b[39;49;00m args.use_4bit:\u001b[37m\u001b[39;49;00m\n",
      "        major, _ = torch.cuda.get_device_capability()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m major >= \u001b[34m8\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * \u001b[34m80\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mYour GPU supports bfloat16: accelerate training with bf16=True\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m * \u001b[34m80\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            bf16=\u001b[34mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set training parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    training_arguments = TrainingArguments(\u001b[37m\u001b[39;49;00m\n",
      "        output_dir=output_dir,\u001b[37m\u001b[39;49;00m\n",
      "        num_train_epochs=args.epochs,\u001b[37m\u001b[39;49;00m\n",
      "        per_device_train_batch_size=args.per_device_train_batch_size,\u001b[37m\u001b[39;49;00m\n",
      "        gradient_accumulation_steps=args.gradient_accumulation_steps,\u001b[37m\u001b[39;49;00m\n",
      "        optim=args.optimizer,\u001b[37m\u001b[39;49;00m\n",
      "        save_steps=args.save_steps,\u001b[37m\u001b[39;49;00m\n",
      "        logging_steps=args.logging_steps,\u001b[37m\u001b[39;49;00m\n",
      "        learning_rate=args.learning_rate,\u001b[37m\u001b[39;49;00m\n",
      "        weight_decay=args.weight_decay,\u001b[37m\u001b[39;49;00m\n",
      "        fp16=fp16,\u001b[37m\u001b[39;49;00m\n",
      "        bf16=bf16,\u001b[37m\u001b[39;49;00m\n",
      "        max_grad_norm=args.max_grad_norm,\u001b[37m\u001b[39;49;00m\n",
      "        max_steps=args.max_steps,\u001b[37m\u001b[39;49;00m\n",
      "        warmup_ratio=args.warmup_ratio,\u001b[37m\u001b[39;49;00m\n",
      "        group_by_length=args.group_by_length,\u001b[37m\u001b[39;49;00m\n",
      "        lr_scheduler_type=args.lr_scheduler_type,\u001b[37m\u001b[39;49;00m\n",
      "        logging_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        report_to=\u001b[33m\"\u001b[39;49;00m\u001b[33mtensorboard\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Set supervised fine-tuning parameters\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.run_experiment == \u001b[33m\"\u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trainer = SFTTrainer(\u001b[37m\u001b[39;49;00m\n",
      "            model=model,\u001b[37m\u001b[39;49;00m\n",
      "            train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            eval_dataset=eval_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            peft_config=lora_config,\u001b[37m\u001b[39;49;00m\n",
      "            dataset_text_field=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            max_seq_length=args.max_seq_length,\u001b[37m\u001b[39;49;00m\n",
      "            tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "            args=training_arguments,\u001b[37m\u001b[39;49;00m\n",
      "            packing=packing,\u001b[37m\u001b[39;49;00m\n",
      "            callbacks=[SageMakerExperimentsCallback(region=args.region)]\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        trainer = SFTTrainer(\u001b[37m\u001b[39;49;00m\n",
      "            model=model,\u001b[37m\u001b[39;49;00m\n",
      "            train_dataset=train_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            eval_dataset=eval_dataset,\u001b[37m\u001b[39;49;00m\n",
      "            peft_config=lora_config,\u001b[37m\u001b[39;49;00m\n",
      "            dataset_text_field=\u001b[33m\"\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            max_seq_length=args.max_seq_length,\u001b[37m\u001b[39;49;00m\n",
      "            tokenizer=tokenizer,\u001b[37m\u001b[39;49;00m\n",
      "            args=training_arguments,\u001b[37m\u001b[39;49;00m\n",
      "            packing=packing\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "    trainer.train()\u001b[37m\u001b[39;49;00m\n",
      "    evaluation_metrics = trainer.evaluate()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining metrics: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrainer.state.log_history\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    model_evaluation(args, trainer.state.log_history)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.merge_weights:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msaving adapter weight combined with the base model weight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# merge adapter weights with base model and save\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# save int 4 model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        new_model = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/lora-adapter-weights\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        trainer.model.save_pretrained(new_model, safe_serialization=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# clear memory\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mdel\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mdel\u001b[39;49;00m trainer\u001b[37m\u001b[39;49;00m\n",
      "        torch.cuda.empty_cache()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        base_model = AutoModelForCausalLM.from_pretrained(\u001b[37m\u001b[39;49;00m\n",
      "            model_name,\u001b[37m\u001b[39;49;00m\n",
      "            low_cpu_mem_usage=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            return_dict=\u001b[34mTrue\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "            torch_dtype=torch.float16,\u001b[37m\u001b[39;49;00m\n",
      "            device_map=device_map,\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        model = PeftModel.from_pretrained(base_model, new_model)\u001b[37m\u001b[39;49;00m\n",
      "        model = model.merge_and_unload()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        model.save_pretrained(output_dir, safe_serialization=\u001b[34mTrue\u001b[39;49;00m, max_shard_size=\u001b[33m\"\u001b[39;49;00m\u001b[33m2GB\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Reload tokenizer to save it\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.pad_token = tokenizer.eos_token\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.padding_side = \u001b[33m\"\u001b[39;49;00m\u001b[33mright\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        tokenizer.save_pretrained(output_dir)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m\u001b[39;49;00m\n",
      "        source_dir = \u001b[33m'\u001b[39;49;00m\u001b[33m./djl-inference/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# copy djl-inference files to model directory\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m f \u001b[35min\u001b[39;49;00m os.listdir(source_dir):\u001b[37m\u001b[39;49;00m\n",
      "            source_f = os.path.join(source_dir, f)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# Copy the files to the destination folder\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            shutil.copy(source_f, output_dir)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33msaving adapter weights only\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        trainer.model.save_pretrained(output_dir, safe_serialization=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    args, _ = parse_arge()\u001b[37m\u001b[39;49;00m\n",
      "    training_function(args)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    main()\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057ab0a-73e0-4d72-ad3f-d47c1f404f97",
   "metadata": {},
   "source": [
    "# Setting up Hyper Parameters for the fine tuning job\n",
    "The following section setup the hyperparameters required for finetuning a QLoRA model. \n",
    "\n",
    "For learn more about the hyperparameter setting for quantization and PEFT, please refer to [this](https://huggingface.co/docs/transformers/main_classes/quantization) and [this](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py) links.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af6492f5-abf7-46c5-8912-48f048d82d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker experiment name: exp-NousResearch-Llama-2-7b-chat-hf\n",
      "SageMaker experiment run name: qlora-finetune-run-2408070215-4854f\n",
      "SageMaker training job name: huggingface-qlora-2024-08-07-02-15-57-4854f\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "experiments_name = f\"exp-{model_id.replace('/', '-')}\"\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}-{rand_id}\"\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}-{rand_id}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'epochs': 2,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 8,                    # Batch size per GPU for training\n",
    "  'per_device_eval_batch_size' : 8,                    # Batch size per GPU for evaluation\n",
    "  'learning_rate' : 2e-4,                              # Initial learning rate (AdamW optimizer)\n",
    "  'optimizer' : \"paged_adamw_32bit\",                   # Optimizer to use\n",
    "  'logging_steps' : 5,                                 # Log every X updates steps\n",
    "  'lora_r': 64,                                        # LoRA attention dimension.\n",
    "  'lora_alpha' : 16,                                   # The alpha parameter for Lora scaling\n",
    "  'lora_dropout' : 0.1,                                # The dropout probability for Lora layers\n",
    "  'use_4bit' : True,                                   # Activate 4-bit precision base model loading\n",
    "  'bnb_4bit_compute_dtype' : \"float16\",                # Compute dtype for 4-bit base models\n",
    "  'bnb_4bit_quant_type' : \"nf4\",                       # Quantization type (fp4 or nf4)\n",
    "  'base_model_group_name' : base_model_pkg_group_name, # Base model registered in SageMaker Model Registry\n",
    "  'region': region,                                    # AWS region where the training is run\n",
    "  'model_eval_s3_loc' : model_eval_s3_loc              # S3 location for uploading the model evaluation metrics\n",
    "}\n",
    "\n",
    "print(f\"SageMaker experiment name: {experiments_name}\")\n",
    "print(f\"SageMaker experiment run name: {run_name}\")\n",
    "print(f\"SageMaker training job name: {job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a2194-f849-4083-818b-365f552e7387",
   "metadata": {},
   "source": [
    "## Run a SageMaker Training Job\n",
    "In this lab, we'll leverage SageMaker Training job to finetune a Llama2-7b model. The training job includes the following information:\n",
    "\n",
    "* The URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n",
    "* The compute resources that you want SageMaker to use for model training. Compute resources are machine learning (ML) compute instances that are managed by SageMaker.\n",
    "* The URL of the S3 bucket where you want to store the output of the job.\n",
    "* The Amazon Elastic Container Registry path where the training code is stored. For more information.\n",
    "\n",
    "In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at /opt/ml/input/data. Then, it starts the training job by running.\n",
    "\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace Estimator`. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In addition, the Estimator manages the infrastructure use. SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`.\n",
    "\n",
    "After the training job, we'll use the estimator object to deploy the model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96027aea-4865-497a-9f21-3d9079b66b06",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2024-08-07-02-15-57-4-2024-08-07-02-16-02-570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-07 02:16:03 Starting - Starting the training job\n",
      "2024-08-07 02:16:03 Pending - Training job waiting for capacity......"
     ]
    }
   ],
   "source": [
    "with Run(\n",
    "    experiment_name=experiments_name,\n",
    "    run_name=run_name,\n",
    "    sagemaker_session=sess\n",
    ") as run:\n",
    "\n",
    "    # create the Estimator\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',         # train script\n",
    "        source_dir='src/train',         # directory which includes all the files needed for training\n",
    "        instance_type='ml.g5.2xlarge', # instances type used for the training job\n",
    "        # instance_type='local_gpu',      # use local \n",
    "        instance_count=1,               # the number of instances used for training\n",
    "        base_job_name=job_name,         # the name of the training job\n",
    "        role=get_execution_role(),      # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "        volume_size=300,    # the size of the EBS volume in GB\n",
    "        transformers_version='4.28.1',    # the transformers version used in the training job\n",
    "        pytorch_version='2.0.0',          # the pytorch_version version used in the training job\n",
    "        py_version='py310',             # the python version used in the training job\n",
    "        hyperparameters= hyperparameters,\n",
    "        environment={ \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "        sagemaker_session=sess,         # specifies a sagemaker session object\n",
    "        output_path=model_output_s3_loc # s3 location for model artifact,\n",
    "    )\n",
    "    \n",
    "    # define a data input dictonary with our uploaded s3 uris\n",
    "    data = { 'training': training_dataset_s3_loc,\n",
    "             'validation': validation_dataset_s3_loc}\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    huggingface_estimator.fit(data, wait=True)\n",
    "    run.log_parameters(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c9a15-e812-43f1-997e-1632ba3a5598",
   "metadata": {},
   "source": [
    "# Deploy the finetuned Llama2 model in SageMaker\n",
    "State-of-the-art deep learning models for applications such as natural language processing (NLP) are large, typically with tens or hundreds of billions of parameters. Larger models are often more accurate, which makes them attractive to machine learning practitioners. However, these models are often too large to fit on a single accelerator or GPU device, making it difficult to achieve low-latency inference. You can avoid this memory bottleneck by using model parallelism techniques to partition a model across multiple accelerators or GPUs.\n",
    "\n",
    "Amazon SageMaker includes specialized deep learning containers (DLCs), libraries, and tooling for model parallelism and large model inference (LMI). In the following sections, you can find resources to get started with LMI on SageMaker.\n",
    "\n",
    "With these DLCs you can use third party libraries such as [DeepSpeed](https://github.com/microsoft/DeepSpeed), [Accelerate](https://huggingface.co/docs/accelerate), and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) to partition model parameters using model parallelism techniques to leverage the memory of multiple GPUs for inference.\n",
    "\n",
    "After the training job, we will deploy the QLoRA finetuned model into SageMaker for inference. In our example, we will also use a Large Model Inference(LMI) container provided by AWS using `DJL Serving` and `DeepSpeed`. Given the llama2-7b model size, this model could fit in a single `ml.g5.2xlarge` instance on AWS SageMaker.\n",
    "\n",
    "### Deep Java Library (DJL) \n",
    "Deep Java Library (DJL) Serving is a high performance universal stand-alone model serving solution powered by DJL. DJL Serving supports loading models trained with a variety of different frameworks. With the SageMaker Python SDK you can use DJL Serving to host large models using backends like DeepSpeed and HuggingFace Accelerate.\n",
    "\n",
    "For more information about using `DJL Serving` model server for hosting LLMs in SageMaker, please refer to the following:\n",
    "\n",
    "* [DeepSpeed and Accelerate](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-deepspeed-djl.html)\n",
    "* [FasterTransformer](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-fastertransformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7604071-1db4-4376-90a4-3e398a9d1769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "llm_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cf047-f104-46c8-bf8f-5132bfb083f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 3600\n",
    "\n",
    "# create HuggingFaceModel with the a DJI image uri\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=huggingface_estimator.model_data,\n",
    "    image_uri=llm_image,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ffe1d-7e7c-4a08-899b-565331c36b93",
   "metadata": {},
   "source": [
    "Trigger a SageMaker deployment by invoking huggingface model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb32f0-2d61-4291-a79b-84fdf0fa2ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name_random_id = uuid.uuid4().hex[:5]\n",
    "endpoint_name = f\"llama2-7b-djl-deepspeed-{endpoint_name_random_id}\"\n",
    "\n",
    "print(f\"endpoint name: {endpoint_name}\")\n",
    "llm = huggingface_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, \n",
    "  endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c38b8-dfa2-44f6-9fae-f96bf35d3883",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test the model\n",
    "In the following section, we'll run a test against the deployed endpoint. Here we format the \n",
    "prompt using the llama2 [standard prompt](https://huggingface.co/blog/llama2#how-to-prompt-llama-2).\n",
    "\n",
    "In the test data, we provide a system prompt along with a question and a few contextual information that might be relevant to the answer. Let's see how the model performs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f64c8-76f1-43ea-b61c-e38c12744f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>\n",
    "[INST] <<SYS>>\n",
    "{{system}}\n",
    "<</SYS>>\n",
    "\n",
    "### Question\n",
    "{{question}}\n",
    "\n",
    "### Context\n",
    "{{context}}[/INST] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dcdb7f-4dcc-42cd-9c93-3133bf19a403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"Given the following context, answer the question as accurately as possible:\"\n",
    "def build_llama2_prompt(message):\n",
    "    question = message['question']\n",
    "    context = message['context']\n",
    "    formatted_message = prompt_template.replace(\"{{system}}\", system_message)\n",
    "    formatted_message = formatted_message.replace(\"{{question}}\", question)\n",
    "    formatted_message = formatted_message.replace(\"{{context}}\", context)\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21659f11-b9ce-44ad-bfb1-20f0a4231d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = {}\n",
    "message['question'] = \"The Oberoi family is part of a hotel company that has a head office in what city?\"\n",
    "message['context'] = \"\"\"The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta. It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two towers that comprises a hotel and the Airlangga Apartment respectively. The hotel was opened in 2005.\n",
    "The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.\n",
    "The Oberoi Group is a hotel company with its head office in Delhi. Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.\n",
    "The 289th Military Police Company was activated on 1 November 1994 and attached to Hotel Company, 3rd Infantry (The Old Guard), Fort Myer, Virginia. Hotel Company is the regiment\\'s specialty company.\\nThe Glennwanis Hotel is a historic hotel in Glennville, Georgia, Tattnall County, Georgia, built on the site of the Hughes Hotel. The hotel is located at 209-215 East Barnard Street. The old Hughes Hotel was built out of Georgia pine circa 1905 and burned in 1920. The Glennwanis was built in brick in 1926. The local Kiwanis club led the effort to get the replacement hotel built, and organized a Glennville Hotel Company with directors being local business leaders. The wife of a local doctor won a naming contest with the name \"Glennwanis Hotel\", a suggestion combining \"Glennville\" and \"Kiwanis\".'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18857e2-4cbc-416f-aaf6-bf5834d47b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = build_llama2_prompt(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415fa153-d9c3-42d8-bea1-b737f486954e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a3e87-3556-4692-8a50-7296b90a2c9c",
   "metadata": {},
   "source": [
    "Run a prediction with inference configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e564c5-e405-4c1f-83c2-2cf73953a895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "  }\n",
    "\n",
    "output = llm.predict({\"text\":input, \"properties\" : params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e346701-2986-44d4-87ce-404b0d2102ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(output['outputs'][0][\"generated_text\"][len(input):]) # automatically removed the bos_token and eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837618c-7676-4361-87cd-0e04b2385014",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d55e04-c5ad-4f7e-b1d8-be7a8369a8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c18db-60f3-46b4-8d4b-ca80201bbb81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
